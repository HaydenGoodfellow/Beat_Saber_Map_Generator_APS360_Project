{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "58c7c12f8aebb3b799780e2a952497b720e238e6f3ceeaa6cb2f46c86be22697"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import time\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"\\\\Map_Processing\")\n",
    "    sys.path.append(module_path+\"\\\\Note_Timing\")\n",
    "\n",
    "# import importlib\n",
    "# importlib.reload(sys.modules['analyze_notes'])\n",
    "# importlib.reload(sys.modules['onset_detection'])\n",
    "\n",
    "from analyze_notes import get_note_placements_by_index \n",
    "from onset_detection import get_onset_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get maps dataframe from the pickle file\n",
    "maps_df = pd.read_pickle(\"../Data_Gather_Filter_Download/downloaded_maps_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Note_Orientation/most_common_placements.pkl', 'rb') as f:\n",
    "    most_common_placements = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================== Dataset Settings ====================================#\n",
    "total_data_size = 15000  # Number of maps to use in across all datasets\n",
    "val_split = 0.15         # Percentage of data put into validation set\n",
    "test_split = 0.15        # Percentage of data put into testing set\n",
    "#==========================================================================================#\n",
    "\n",
    "# Split our data into training and test/val which we will split again\n",
    "train_df, val_test_df = train_test_split(maps_df[:total_data_size], test_size=val_split + test_split)\n",
    "\n",
    "# Split the validation and testing data apart into their own respective sets\n",
    "val_df, test_df = train_test_split(val_test_df, test_size=(test_split/(val_split + test_split)))\n",
    "\n",
    "print(\"Number of training maps: {}. Numer of validation maps: {}. Number of testing maps: {}\".format(len(train_df), len(val_df), len(test_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class beatmap_generator(nn.Module):\n",
    "    def __init__(self, input_size, output_size, seq_size, hidden_size, num_layers=1, dropout=0):\n",
    "        super(beatmap_generator, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.seq_size = seq_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # LSTM model\n",
    "        self.net = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "        # Converts output back to valid index into most common notes\n",
    "        self.decoder = nn.Linear(hidden_size * seq_size, output_size)\n",
    "    \n",
    "    def init_hidden_layer(self, batch_size):\n",
    "        self.batch_size = batch_size # Have to set it here\n",
    "        hidden_init = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)\n",
    "        cell_init = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)\n",
    "        self.hidden = (hidden_init, cell_init)\n",
    "\n",
    "    def forward(self, data):\n",
    "        output, self.hidden = self.net(data, self.hidden)   # get the next output and hidden state\n",
    "        output = output.contiguous().view(data.size(0), -1)\n",
    "        output = self.decoder(output)                       # predict distribution over next tokens\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================== Model Settings =====================================#\n",
    "input_size = 25         # Number of features in input\n",
    "output_size = 2001      # Number of possible outputs for model\n",
    "seq_size = 512          # Length of note sequences passed into model\n",
    "hidden_size = 128       # Number of nodes in hidden layer\n",
    "num_layers = 2          # Number of LSTM layers for stacked LSTM\n",
    "dropout = 0.24          # Probability of droping weights in the dropout layer\n",
    "#==========================================================================================#\n",
    "\n",
    "bm_gen = beatmap_generator(input_size, output_size, seq_size, hidden_size, num_layers, dropout)\n",
    "# Set it to use the GPU\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"CUDA available, using GPU\")\n",
    "    bm_gen = bm_gen.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from tutorial 5 and 6 code and tutorial 3 and 4\n",
    "def train_network(model, train_df, val_df, num_epochs=5, learning_rate=0.04, batch_size=16):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    num_trained = 0\n",
    "    losses, train_acc, valid_acc = np.zeros(num_epochs), np.zeros(num_epochs), np.zeros(num_epochs)\n",
    "    num_songs = 0\n",
    "    epochs = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for beatmap in train_df.itertuples():\n",
    "            start_time = time.time()\n",
    "            diff = beatmap.difficulty\n",
    "            diff = diff[:1].upper() + diff[1:] # Capitialize the first character only\n",
    "            if 'extensions' in beatmap.requirements:\n",
    "                continue # Catches a few maps that shouldn't have made it here\n",
    "            # See if the features have already been calculated\n",
    "            try:\n",
    "                folder_name = beatmap.file_path.rsplit('/', 1)[-1]\n",
    "                folder_name = folder_name.split('.')[0]\n",
    "                folder_p = Path('Saved_Features/v{}_{}/'.format(features_version, folder_name))\n",
    "                file_p = folder_p / '{}.npy'.format(diff)\n",
    "                print(file_p)\n",
    "                features_dict = np.load(file_p, allow_pickle=True)\n",
    "                print(\"Successfully opened file for {} on {} difficulty\".format(beatmap.song_name, diff))\n",
    "                features = features_dict.item().get('features')\n",
    "                padded_placements = features_dict.item().get('placements')\n",
    "            \n",
    "            except Exception as e: # If they haven't been then calculate them now\n",
    "                print(\"Calculating features and placements for {} on {} difficulty. Exception: {}\".format(beatmap.song_name, diff, e))\n",
    "                with ZipFile('../Data_Gather_Filter_Download/{}'.format(beatmap.file_path)) as folder:\n",
    "                    filenames = folder.namelist()\n",
    "                    with folder.open('{}.dat'.format(diff)) as dat_file:\n",
    "                        dat_json = json.load(dat_file)\n",
    "                        placements = get_note_placements_by_index(dat_json, most_common_placements)\n",
    "                    song_path = list(filter(lambda x: re.match(r'(^.+\\.(egg|ogg|mp4|mp3))', x, flags=re.I), filenames))[0]\n",
    "                    folder.extract(song_path)\n",
    "                    features, padded_placements = get_features(song_path, bpm=beatmap.bpm, placements=placements, \n",
    "                                                               save_data=True, difficulty=diff, folder_path=beatmap.file_path)\n",
    "                    os.remove(song_path)\n",
    "            # Create sequence arrays for different permutations\n",
    "            feature_sequences, placements_for_seq = split_into_sequences(features, padded_placements, 512)\n",
    "            feature_tensor = torch.from_numpy(feature_sequences).float()\n",
    "            placements_for_seq_tensor = torch.from_numpy(placements_for_seq).long()\n",
    "            # Create the data loader we will use\n",
    "            train_loader = torch.utils.data.DataLoader(feature_tensor, batch_size=batch_size, num_workers=num_workers)\n",
    "            label_loader = torch.utils.data.DataLoader(placements_for_seq_tensor, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "            \n",
    "\n",
    "            for feature_batch, target_batch in zip(train_loader, label_loader):\n",
    "                if torch.cuda.is_available():\n",
    "                    feature_batch = feature_batch.cuda()\n",
    "                    target_batch = target_batch.cuda()\n",
    "                model.init_hidden_layer(feature_batch.size(0))\n",
    "                output = model(feature_batch)\n",
    "                # print('output shape: {}'.format(output.shape))\n",
    "                # output = output.view(output.size(0) * output.size(1), output.size(2))\n",
    "                # print('output shape: {}'.format(output.shape))\n",
    "                # print('target shape: {}'.format(target_batch.shape))\n",
    "                # print('Max in target {}, min in target {}'.format(torch.argmax(target_batch), torch.argmin(target_batch)))\n",
    "                loss = criterion(output, target_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            end_time = time.time()\n",
    "            num_trained += 1\n",
    "            print(\"Done up to {} songs. Time taken for song: {:.2f} seconds\".format(num_trained, end_time - start_time))\n",
    "        losses[epoch] = float(loss)    \n",
    "        epochs.append(epoch)\n",
    "        train_acc[epoch] = get_accuracy(model, train_iter)\n",
    "        valid_acc[epoch] = get_accuracy(model, valid_iter)\n",
    "        print(\"Epoch %d; Loss %f; Train Acc %f; Val Acc %f\" % (\n",
    "            epoch+1, loss, train_acc[epoch], valid_acc[epoch]))\n",
    "\n",
    "    # plotting\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(losses, label=\"Train\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(epochs, train_acc, label=\"Train\")\n",
    "    plt.plot(epochs, valid_acc, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    torch.save(model, './beatmap_gen_model_v{}'.format(features_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#==================================== Training Settings ===================================#\n",
    "learning_rate = 0.04    # Learning rate\n",
    "num_epochs = 10         # Number of epochs\n",
    "batch_size = 32         # Number of sequences to batch together\n",
    "num_workers = 4         # Number of workers to load the data\n",
    "features_version = 4    # If I make any large changes that require re-calculating features\n",
    "#==========================================================================================#\n",
    "\n",
    "\n",
    "train_network(bm_gen, train_df, val_df, num_epochs=num_epochs, learning_rate=learning_rate, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(song_path, bpm, placements=None, save_data=False, difficulty='expertPlus', version=features_version, folder_path=''):\n",
    "    beat_frames, sr, melspectrogram, chromagram = get_audio_data(song_path, bpm)\n",
    "    quarter_frames = np.array([]) # Array to store data for every quarter note\n",
    "    index = 0\n",
    "    while index < len(beat_frames) - 1:\n",
    "        quarter_frames = np.append(quarter_frames, np.arange(beat_frames[index], beat_frames[index + 1], \n",
    "                                                            round((beat_frames[index + 1] - beat_frames[index]) / 4))[1:4])\n",
    "        index += 1\n",
    "    \n",
    "    beat_frames_with_quarter = np.concatenate((np.array([0]), beat_frames, quarter_frames), axis=0)\n",
    "    beat_frames_with_quarter.sort() # We appended the quarter notes so we need to sort them into right spots\n",
    "    beat_frames_with_quarter = beat_frames_with_quarter.astype(int)\n",
    "\n",
    "    beat_frames_with_mel = librosa.util.sync(melspectrogram, beat_frames_with_quarter, aggregate=np.median)\n",
    "    beat_frames_with_chroma = librosa.util.sync(chromagram, beat_frames_with_quarter, aggregate=np.median)\n",
    "\n",
    "    # Want to extend it to 1/16 beats as 1/4 beats limits the max NPS by a lot\n",
    "    # Using pandas to easily extend to 1/16 beats\n",
    "    beat_num_with_16th_notes = np.arange(0, len(beat_frames) - 1, 1.0 / 16.0)\n",
    "    beat_num = np.arange(0, len(beat_frames), 1)\n",
    "    beats_num_df = pd.concat([pd.Series(beat_num, name='beat_num', dtype=int), pd.Series(beat_frames, name='beat_frame', dtype=int)], axis=1)\n",
    "    mel_beats_df = pd.concat([pd.Series(beat_frames_with_quarter, name='quater_beat_frame'), pd.DataFrame(beat_frames_with_mel.T)], axis=1)\n",
    "    chroma_beats_df = pd.concat([pd.Series(beat_frames_with_quarter, name='quater_beat_frame'), pd.DataFrame(beat_frames_with_chroma.T)], axis=1)\n",
    "\n",
    "    # Dataframe with the beat number, frame number, mel data, and chroma data for the beat\n",
    "    beats_df = beats_num_df.merge(mel_beats_df, how='outer', left_on='beat_frame', right_on='quater_beat_frame', sort=True)\n",
    "\n",
    "    # Only need the quarter beat frames\n",
    "    beats_df = beats_df.drop(columns=['beat_frame']) \n",
    "    beats_df = beats_df.merge(chroma_beats_df, how='outer', on='quater_beat_frame', sort=True)\n",
    "\n",
    "    # Removes the NaNs from the beat number column for next merge\n",
    "    beats_df.interpolate(inplace=True) \n",
    "\n",
    "    # Expand it to be 1/16 beats. Doing it in 1/4 beats saves a lot of computation with sync\n",
    "    beats_num_16th_notes_df = pd.DataFrame(beat_num_with_16th_notes, columns=['beat_num'])\n",
    "    beats_df = beats_df.merge(beats_num_16th_notes_df, how='outer', on='beat_num', sort=True)\n",
    "\n",
    "    # Interpolate the quarter beat frame so we can more accuractly place the placements\n",
    "    beats_df['quater_beat_frame'].interpolate(inplace=True) \n",
    "    beats_df['quater_beat_frame'] = beats_df['quater_beat_frame'].round()\n",
    "    beats_df = beats_df.fillna(method='pad') # Forward fill\n",
    "\n",
    "    # Add placement column to store what note type is at that time\n",
    "    beats_df.insert(1, 'placement', 0)\n",
    "    # Add column for time since last note since this helps dicate what placements should be done\n",
    "    beats_df.insert(3, 'time_since_last_note', 0.0)\n",
    "\n",
    "    # Now the computed audio values for each quarter note are spread among 1/16 notes so we can use sequences of 1/16 notes\n",
    "    for timing, placement in placements.items():\n",
    "        time_in_frames = librosa.core.time_to_frames(beat_to_time(timing, bpm), sr=sr)\n",
    "        try:\n",
    "            matching_frames = beats_df.loc[beats_df['quater_beat_frame'] == time_in_frames, 'placement']\n",
    "            if len(matching_frames) == 1:\n",
    "                beats_df.loc[beats_df['quater_beat_frame'] == time_in_frames, 'placement'] = placement\n",
    "            else: # Must be more than one quater beat on that frame\n",
    "                placement_arr = [placement]\n",
    "                placement_arr.extend([0] * (len(matching_frames) - 1))\n",
    "                beats_df.loc[beats_df['quater_beat_frame'] == time_in_frames, 'placement'] = placement_arr\n",
    "        except: # No exact note timing match\n",
    "            # Subtract the value and find the one closest to 0\n",
    "            closest_index = beats_df['quater_beat_frame'].sub(time_in_frames).abs().idxmin()\n",
    "            beats_df[closest_index]['placement'] = placement\n",
    "    # Set the placements with 16 to 0 as they represent the same board but are easier to distinguish\n",
    "    beats_df.loc[beats_df['placement'] == 16, 'placement'] = 0\n",
    "\n",
    "    # Extract from pandas into list which we can turn into tensor later\n",
    "    features = []\n",
    "    last_note_time = 0\n",
    "    for i in range(len(beats_df.index)):\n",
    "        # Set the time since the last note\n",
    "        time_in_s = librosa.core.frames_to_time(beats_df.at[i, 'quater_beat_frame'], sr=sr)\n",
    "        if beats_df.at[i, 'placement'] != 0 and beats_df.at[i, 'placement'] != 16:\n",
    "            last_note_time = time_in_s\n",
    "        if last_note_time != 0:\n",
    "            beats_df.at[i, 'time_since_last_note'] = (time_in_s - last_note_time)\n",
    "        # Extract the data from this row\n",
    "        features.append(beats_df.iloc[i, 3:].tolist())\n",
    "\n",
    "    # Convert to numpy arrays for easier slicing later\n",
    "    features = np.asarray(features)\n",
    "    # Placements made by the human mapper\n",
    "    human_placements = beats_df['placement'].to_numpy()\n",
    "\n",
    "    if save_data:\n",
    "        feat_dict = {'features' : features, 'placements' : human_placements}\n",
    "        folder_name = folder_path.rsplit('/', 1)[-1]\n",
    "        folder_name = folder_name.split('.')[0]\n",
    "        path = 'Saved_Features/v{}_{}/'.format(version, folder_name)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        np.save(os.path.join(path, '{}.npy'.format(difficulty)), feat_dict)\n",
    "\n",
    "    return features, human_placements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_data(song_path, bpm):\n",
    "    y, sr = librosa.load(song_path)\n",
    "    length = y.shape[0] / sr # Song length according to librosa in secs (doesn't match given length for some reason)\n",
    "    y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
    "    tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr, trim=False, bpm=bpm)\n",
    "    # Reduce n_mels to avoid empty filters in mel frequency basis and to match the size of chrormagram\n",
    "    melspectrogram = librosa.feature.melspectrogram(y=y_percussive, sr=sr, n_mels=12, fmax=65.4)\n",
    "    chromagram = librosa.feature.chroma_cqt(y=y_harmonic, sr=sr)\n",
    "    return beat_frames, sr, melspectrogram, chromagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sequences(features, placements, seq_len):\n",
    "    feature_sequences = []\n",
    "    placements_for_seq = []\n",
    "    for i in range(len(features)):\n",
    "        end_index = i + seq_len\n",
    "        if end_index > len(features):\n",
    "            break\n",
    "        feature_seq = features[i:end_index, :] \n",
    "        placement_for_seq = placements[i]\n",
    "        feature_sequences.append(feature_seq)\n",
    "        placements_for_seq.append(placement_for_seq)\n",
    "    return np.array(feature_sequences), np.array(placements_for_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(net, data):\n",
    "    correct, total = 0, 0\n",
    "    for sms, labels in data:\n",
    "        output = net(sms[0])\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        total += labels.shape[0]\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to do some conversions\n",
    "# Time in seconds to beat number\n",
    "def time_to_beat(note_time, bpm):\n",
    "    return (note_time / 60) * bpm\n",
    "\n",
    "# Beat number to seconds\n",
    "def beat_to_time(beat_time, bpm):\n",
    "    return (beat_time / bpm) * 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns file path to folder containing all files needed to play song made by model\n",
    "def get_map_from_song(song_path, model, seq_len=512, output_file_path='Expert.dat', start_time=2, bpm=0):\n",
    "    # Get the onset times where we will place notes\n",
    "    onset_times = get_onset_times(song_path, min_sep=0.1)\n",
    "    num_before = len(onset_times)\n",
    "    onset_times = np.delete(onset_times, np.where(onset_times <= start_time))\n",
    "    print(\"Removed {} onset times for being before the specified start time\".format(num_before - len(onset_times)))\n",
    "    # If the bpm is not provided then we calculate it ourselves\n",
    "    if bpm == 0:\n",
    "        y, samp_rate = librosa.load(song_file)\n",
    "        bpm = librosa.beat.tempo(y=y, sr=samp_rate)\n",
    "        print(\"Got a bpm of {}\".format(bpm))\n",
    "    # Determine the notes we should place\n",
    "    beats_df, sr = generate_placements(song_path, model, bpm, onset_times, seq_len)\n",
    "\n",
    "    notes_as_json = convert_model_placements_to_valid_json(beats_df, most_common_placements, sr, bpm)\n",
    "    with open(output_file_path, 'w') as dat_file:\n",
    "        dat_data = {\"_version\": \"2.2.0\",\n",
    "                    \"_customData\": {\n",
    "                        \"_time\": '',\n",
    "                        \"_BPMChanges\": [],\n",
    "                        \"_bookmarks\": []\n",
    "                        },\n",
    "                    \"_events\": [],\n",
    "                    \"_notes\": notes_as_json,\n",
    "                    \"_obstacles\": [],\n",
    "                    \"_waypoints\": []\n",
    "                    }\n",
    "        json.dump(dat_data, dat_file)\n",
    "    \n",
    "    print(\"Number of notes placed: {}\\nApprox. notes per second: {}\".format(\n",
    "            len(notes_as_json),\n",
    "            len(notes_as_json) / np.amax(onset_times)\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_map_from_song('redo.egg', bm_gen, seq_len=seq_size, bpm=190)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_placements(song_path, model, bpm, onset_times, seq_len, difficulty='expertPlus', version=features_version):\n",
    "    beat_frames, sr, melspectrogram, chromagram = get_audio_data(song_path, bpm)\n",
    "    quarter_frames = np.array([]) # Array to store data for every quarter note\n",
    "    index = 0\n",
    "    while index < len(beat_frames) - 1:\n",
    "        quarter_frames = np.append(quarter_frames, np.arange(beat_frames[index], beat_frames[index + 1], \n",
    "                                                            round((beat_frames[index + 1] - beat_frames[index]) / 4))[1:4])\n",
    "        index += 1\n",
    "    \n",
    "    beat_frames_with_quarter = np.concatenate((np.array([0]), beat_frames, quarter_frames), axis=0)\n",
    "    beat_frames_with_quarter.sort() # We appended the quarter notes so we need to sort them into right spots\n",
    "    beat_frames_with_quarter = beat_frames_with_quarter.astype(int)\n",
    "\n",
    "    beat_frames_with_mel = librosa.util.sync(melspectrogram, beat_frames_with_quarter, aggregate=np.median)\n",
    "    beat_frames_with_chroma = librosa.util.sync(chromagram, beat_frames_with_quarter, aggregate=np.median)\n",
    "\n",
    "    # Want to extend it to 1/16 beats as 1/4 beats limits the max NPS by a lot\n",
    "    # Using pandas to \"easily\" extend to 1/16 beats\n",
    "    beat_num_with_16th_notes = np.arange(0, len(beat_frames) - 1, 1.0 / 16.0)\n",
    "    beat_num = np.arange(0, len(beat_frames), 1)\n",
    "    beats_num_df = pd.concat([pd.Series(beat_num, name='beat_num', dtype=int), pd.Series(beat_frames, name='beat_frame', dtype=int)], axis=1)\n",
    "    mel_beats_df = pd.concat([pd.Series(beat_frames_with_quarter, name='quater_beat_frame'), pd.DataFrame(beat_frames_with_mel.T)], axis=1)\n",
    "    chroma_beats_df = pd.concat([pd.Series(beat_frames_with_quarter, name='quater_beat_frame'), pd.DataFrame(beat_frames_with_chroma.T)], axis=1)\n",
    "\n",
    "    # Dataframe with the beat number, frame number, mel data, and chroma data for the beat\n",
    "    beats_df = beats_num_df.merge(mel_beats_df, how='outer', left_on='beat_frame', right_on='quater_beat_frame', sort=True)\n",
    "\n",
    "    # Only need the quarter beat frames\n",
    "    beats_df = beats_df.drop(columns=['beat_frame']) \n",
    "    beats_df = beats_df.merge(chroma_beats_df, how='outer', on='quater_beat_frame', sort=True)\n",
    "\n",
    "    # Removes the NaNs from the beat number column for next merge\n",
    "    beats_df.interpolate(inplace=True) \n",
    "\n",
    "    # Expand it to be 1/16 beats. Doing it in 1/4 beats saves a lot of computation with sync\n",
    "    beats_num_16th_notes_df = pd.DataFrame(beat_num_with_16th_notes, columns=['beat_num'])\n",
    "    beats_df = beats_df.merge(beats_num_16th_notes_df, how='outer', on='beat_num', sort=True)\n",
    "\n",
    "    # Interpolate the quarter beat frame so we can more accuractly place the placements\n",
    "    beats_df['quater_beat_frame'].interpolate(inplace=True) \n",
    "    beats_df['quater_beat_frame'] = beats_df['quater_beat_frame'].round()\n",
    "    beats_df = beats_df.fillna(method='pad') # Forward fill\n",
    "\n",
    "    # Add placement column to store what note type is at that time\n",
    "    beats_df.insert(1, 'placement', 0)\n",
    "    # Add column for time since last note since this helps dicate what placements should be done\n",
    "    beats_df.insert(3, 'time_since_last_note', 0.0)\n",
    "    print(\"At timing\")\n",
    "    for timing in onset_times:\n",
    "        time_in_frames = librosa.core.time_to_frames(timing, sr=sr)\n",
    "        try:\n",
    "            matching_frames = beats_df.loc[beats_df['quater_beat_frame'] == time_in_frames, 'placement']\n",
    "            if len(matching_frames) == 1:\n",
    "                beats_df.loc[beats_df['quater_beat_frame'] == time_in_frames, 'placement'] = 9999\n",
    "            else: # Must be more than one quater beat on that frame\n",
    "                placement_arr = [9999]\n",
    "                placement_arr.extend([0] * (len(matching_frames) - 1))\n",
    "                beats_df.loc[beats_df['quater_beat_frame'] == time_in_frames, 'placement'] = placement_arr\n",
    "        except: # No exact note timing match\n",
    "            # Subtract the value and find the one closest to 0\n",
    "            closest_index = beats_df['quater_beat_frame'].sub(time_in_frames).abs().idxmin()\n",
    "            beats_df[closest_index]['placement'] = 9999            \n",
    "    \n",
    "    last_note_time = 0\n",
    "    for i in range(len(beats_df.index)):\n",
    "        # Set the time since the last note\n",
    "        time_in_s = librosa.core.frames_to_time(beats_df.at[i, 'quater_beat_frame'], sr=sr)\n",
    "        if beats_df.at[i, 'placement'] != 0 and beats_df.at[i, 'placement'] != 16:\n",
    "            last_note_time = time_in_s\n",
    "        if last_note_time != 0:\n",
    "            beats_df.at[i, 'time_since_last_note'] = (time_in_s - last_note_time)\n",
    "    \n",
    "    model.init_hidden_layer(batch_size=1)\n",
    "    placement_indicies = list(np.where(beats_df['placement'] == 9999)[0])\n",
    "    for index in placement_indicies:\n",
    "        seq = []\n",
    "        start_index = index - seq_len\n",
    "        # print(type(index))\n",
    "        # print(type(placement_indicies))\n",
    "        # print(type(start_index))\n",
    "        # print(start_index)\n",
    "        if start_index <= 0:\n",
    "            num_padding = abs(start_index)\n",
    "            seq.extend([[0] * len(beats_df.iloc[i, 3:].tolist())] * num_padding)\n",
    "            start_index = 0\n",
    "        relev_df = beats_df.iloc[start_index:index, 3:]\n",
    "        for ind, d in relev_df.iterrows():\n",
    "            seq.append(d.tolist())\n",
    "        # print(seq)\n",
    "        seq = np.array(seq)\n",
    "        feature_tensor = torch.from_numpy(seq).float()\n",
    "        if torch.cuda.is_available():\n",
    "            feature_tensor = feature_tensor.cuda()\n",
    "        output = model(feature_tensor.unsqueeze(0))\n",
    "        # print(output.shape)\n",
    "        _, chosen_placements = torch.topk(output, 3, 1)\n",
    "        # print(chosen_placements)\n",
    "        # print(chosen_placements.shape)\n",
    "        chosen_placement = chosen_placements[0][0] if chosen_placements[0][0] != 0 else chosen_placements[0][1]\n",
    "        beats_df.at[index, 'placement'] = chosen_placement\n",
    "\n",
    "    return beats_df, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_model_placements_to_valid_json(beats_df, most_common_placements, sr, bpm):\n",
    "    list_of_jsons = []\n",
    "    beats_with_notes = beats_df.loc[beats_df['placement'] != 0]\n",
    "    for index, beat in beats_with_notes.iterrows():\n",
    "        placement = beat['placement']\n",
    "        time_in_beat = time_to_beat(librosa.core.frames_to_time(beat['quater_beat_frame'], sr=sr), bpm)\n",
    "        placement_info = most_common_placements[int(placement)]\n",
    "        for i in range(len(placement_info)):\n",
    "            if placement_info[i] != 0:\n",
    "                val = placement_info[i]\n",
    "                # 0 - Red, 1 - Blue \n",
    "                colour = 0 if val < 10 else 1\n",
    "                note_dir = val - (colour * 9) - 1\n",
    "                col = i % 4\n",
    "                row = (i - col) / 4\n",
    "                note_json = {\"_time\": time_in_beat,\n",
    "                            \"_lineIndex\": col,\n",
    "                            \"_lineLayer\": row,\n",
    "                            \"_type\": colour,\n",
    "                            \"_cutDirection\": note_dir}\n",
    "                list_of_jsons.append(note_json)\n",
    "    return list_of_jsons"
   ]
  }
 ]
}