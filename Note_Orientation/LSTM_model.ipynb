{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "58c7c12f8aebb3b799780e2a952497b720e238e6f3ceeaa6cb2f46c86be22697"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import time\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"\\\\Map_Processing\")\n",
    "    sys.path.append(module_path+\"\\\\Note_Timing\")\n",
    "\n",
    "# import importlib\n",
    "# importlib.reload(sys.modules['analyze_notes'])\n",
    "# importlib.reload(sys.modules['onset_detection'])\n",
    "\n",
    "from analyze_notes import get_note_placements_by_index \n",
    "from onset_detection import get_onset_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get maps dataframe from the pickle file\n",
    "maps_df = pd.read_pickle(\"../Data_Gather_Filter_Download/downloaded_maps_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Note_Orientation/most_common_placements.pkl', 'rb') as f:\n",
    "    most_common_placements = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of training maps: 11250. Numer of validation maps: 3000. Number of testing maps: 750\n"
     ]
    }
   ],
   "source": [
    "#==================================== Dataset Settings ====================================#\n",
    "# Note: We have to use a subset of all our songs as 15k maps would take days to train\n",
    "total_data_size = 15000 # Number of maps to use in across all datasets\n",
    "val_split = 0.2         # Percentage of data put into validation set\n",
    "test_split = 0.05       # Percentage of data put into testing set\n",
    "#==========================================================================================#\n",
    "\n",
    "# Split our data into training and test/val which we will split again\n",
    "train_df, val_test_df = train_test_split(maps_df[:total_data_size], test_size=val_split + test_split)\n",
    "\n",
    "# Split the validation and testing data apart into their own respective sets\n",
    "val_df, test_df = train_test_split(val_test_df, test_size=(test_split/(val_split + test_split)))\n",
    "\n",
    "print(\"Number of training maps: {}. Numer of validation maps: {}. Number of testing maps: {}\".format(len(train_df), len(val_df), len(test_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class beatmap_generator(nn.Module):\n",
    "    def __init__(self, input_size, output_size, seq_size, hidden_size, num_layers=1, dropout=0):\n",
    "        super(beatmap_generator, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.seq_size = seq_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # LSTM model\n",
    "        self.net = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "        # Converts output back to valid index into most common notes\n",
    "        self.decoder = nn.Linear(hidden_size * seq_size, output_size)\n",
    "    \n",
    "    def init_hidden_layer(self, batch_size):\n",
    "        self.batch_size = batch_size # Have to set it here\n",
    "        hidden_init = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)\n",
    "        cell_init = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)\n",
    "        self.hidden = (hidden_init, cell_init)\n",
    "\n",
    "    def forward(self, data):\n",
    "        output, self.hidden = self.net(data, self.hidden)   # get the next output and hidden state\n",
    "        output = output.contiguous().view(data.size(0), -1)\n",
    "        output = self.decoder(output)                       # predict distribution over next tokens\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CUDA available, using GPU\n"
     ]
    }
   ],
   "source": [
    "#===================================== Model Settings =====================================#\n",
    "input_size = 25                             # Number of features in input\n",
    "output_size = len(most_common_placements)   # Number of possible outputs for model\n",
    "seq_size = 512                              # Length of note sequences passed into model\n",
    "hidden_size = 128                           # Number of nodes in hidden layer\n",
    "num_layers = 2                              # Number of LSTM layers for stacked LSTM\n",
    "dropout = 0                                 # Probability of droping weights in the dropout layer\n",
    "#==========================================================================================#\n",
    "\n",
    "bm_gen = beatmap_generator(input_size, output_size, seq_size, hidden_size, num_layers, dropout)\n",
    "# Set it to use the GPU\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"CUDA available, using GPU\")\n",
    "    bm_gen = bm_gen.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from tutorial 5 and 6 code and tutorial 3 and 4\n",
    "def train_network(model, train_df, val_df, num_epochs=5, learning_rate=learning_rate, batch_size=16):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    losses, train_acc, valid_acc = np.zeros(num_epochs), np.zeros(num_epochs), np.zeros(num_epochs)\n",
    "    num_songs = 0\n",
    "    epochs = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for beatmap in train_df.itertuples():\n",
    "            start_time = time.time()\n",
    "            diff = beatmap.difficulty\n",
    "            diff = diff[:1].upper() + diff[1:] # Capitialize the first character only\n",
    "            if 'extensions' in beatmap.requirements:\n",
    "                continue # Catches a few maps that shouldn't have made it here\n",
    "            # See if the features have already been calculated\n",
    "            try:\n",
    "                folder_name = beatmap.file_path.rsplit('/', 1)[-1]\n",
    "                folder_name = folder_name.split('.')[0]\n",
    "                folder_p = Path('Saved_Features/v{}_{}/'.format(features_version, folder_name))\n",
    "                file_p = folder_p / '{}.npy'.format(diff)\n",
    "                print(file_p)\n",
    "                features_dict = np.load(file_p, allow_pickle=True)\n",
    "                print(\"Successfully opened file for {} on {} difficulty\".format(beatmap.song_name, diff))\n",
    "                features = features_dict.item().get('features')\n",
    "                padded_placements = features_dict.item().get('placements')\n",
    "            \n",
    "            except Exception as e: # If they haven't been then calculate them now\n",
    "                print(\"Calculating features and placements for {} on {} difficulty. Exception: {}\".format(beatmap.song_name, diff, e))\n",
    "                with ZipFile('../Data_Gather_Filter_Download/{}'.format(beatmap.file_path)) as folder:\n",
    "                    filenames = folder.namelist()\n",
    "                    with folder.open('{}.dat'.format(diff)) as dat_file:\n",
    "                        dat_json = json.load(dat_file)\n",
    "                        placements = get_note_placements_by_index(dat_json, most_common_placements)\n",
    "                    song_path = list(filter(lambda x: re.match(r'(^.+\\.(egg|ogg|mp4|mp3))', x, flags=re.I), filenames))[0]\n",
    "                    folder.extract(song_path)\n",
    "                    features, padded_placements = get_features(song_path, bpm=beatmap.bpm, placements=placements, \n",
    "                                                               save_data=True, difficulty=diff, folder_path=beatmap.file_path)\n",
    "                    os.remove(song_path)\n",
    "            # Create sequence arrays for different permutations\n",
    "            feature_sequences, placements_for_seq = split_into_sequences(features, padded_placements, 512)\n",
    "            feature_tensor = torch.from_numpy(feature_sequences).float()\n",
    "            placements_for_seq_tensor = torch.from_numpy(placements_for_seq).long()\n",
    "            # Create the data loader we will use\n",
    "            train_loader = torch.utils.data.DataLoader(feature_tensor, batch_size=batch_size, num_workers=num_workers)\n",
    "            label_loader = torch.utils.data.DataLoader(placements_for_seq_tensor, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "            \n",
    "            print(\"Looping over {} batches\".format(len(train_loader)))\n",
    "            for feature_batch, target_batch in zip(train_loader, label_loader):\n",
    "                if torch.cuda.is_available():\n",
    "                    feature_batch = feature_batch.cuda()\n",
    "                    target_batch = target_batch.cuda()\n",
    "                model.init_hidden_layer(feature_batch.size(0))\n",
    "                output = model(feature_batch)\n",
    "                # print('output shape: {}'.format(output.shape))\n",
    "                # output = output.view(output.size(0) * output.size(1), output.size(2))\n",
    "                # print('output shape: {}'.format(output.shape))\n",
    "                # print('target shape: {}'.format(target_batch.shape))\n",
    "                # print('Max in target {}, min in target {}'.format(torch.argmax(target_batch), torch.argmin(target_batch)))\n",
    "                loss = criterion(output, target_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            end_time = time.time()\n",
    "            print(\"Time taken for song: {:.2f} seconds\".format(end_time - start_time))\n",
    "        losses[epoch] = float(loss)    \n",
    "        epochs.append(epoch)\n",
    "        train_acc[epoch] = get_accuracy(model, train_iter)\n",
    "        valid_acc[epoch] = get_accuracy(model, valid_iter)\n",
    "        print(\"Epoch %d; Loss %f; Train Acc %f; Val Acc %f\" % (\n",
    "            epoch+1, loss, train_acc[epoch], valid_acc[epoch]))\n",
    "\n",
    "    # plotting\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(losses, label=\"Train\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(epochs, train_acc, label=\"Train\")\n",
    "    plt.plot(epochs, valid_acc, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ory: 'Saved_Features\\\\v1_(23d7)_Red_Hot_Chili_Peppers_-_Otherside\\\\Expert.npy'\n",
      "Looping over 483 batches\n",
      "Time taken for song: 53.60 seconds\n",
      "Saved_Features\\v1_(50ec)_Jump_Training_-_V1\\ExpertPlus.npy\n",
      "Calculating features and placements for Jump Training - V1 on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(50ec)_Jump_Training_-_V1\\\\ExpertPlus.npy'\n",
      "Looping over 402 batches\n",
      "Time taken for song: 50.81 seconds\n",
      "Saved_Features\\v1_(b2f)_Koi_ha_tenshi_no_chaimu_kara_-_Yukari_Tamura\\Expert.npy\n",
      "Calculating features and placements for Koi ha tenshi no chaimu kara - Yukari Tamura on Expert difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(b2f)_Koi_ha_tenshi_no_chaimu_kara_-_Yukari_Tamura\\\\Expert.npy'\n",
      "Looping over 202 batches\n",
      "Time taken for song: 28.15 seconds\n",
      "Saved_Features\\v1_(b6a2)_DECO27_-_Nocturnal_Kids_ft\\ExpertPlus.npy\n",
      "Calculating features and placements for DECO*27 - Nocturnal Kids [ft. Hatsune Miku] on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(b6a2)_DECO27_-_Nocturnal_Kids_ft\\\\ExpertPlus.npy'\n",
      "Looping over 474 batches\n",
      "Time taken for song: 52.86 seconds\n",
      "Saved_Features\\v1_(d6c8)_Like_Its_Over_-_Jai_Wolf\\ExpertPlus.npy\n",
      "Calculating features and placements for Like It's Over - Jai Wolf on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(d6c8)_Like_Its_Over_-_Jai_Wolf\\\\ExpertPlus.npy'\n",
      "Looping over 899 batches\n",
      "Time taken for song: 86.00 seconds\n",
      "Saved_Features\\v1_(66f0)_Apashe_x_Jaykode_-_ANNIHILATION\\ExpertPlus.npy\n",
      "Calculating features and placements for Apashe x Jaykode - ANNIHILATION on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(66f0)_Apashe_x_Jaykode_-_ANNIHILATION\\\\ExpertPlus.npy'\n",
      "Looping over 429 batches\n",
      "Time taken for song: 44.92 seconds\n",
      "Saved_Features\\v1_(6d70)_Spooky_Scary_Skeletons_-_Andrew_Gold_(Updated)\\ExpertPlus.npy\n",
      "Calculating features and placements for Spooky Scary Skeletons - Andrew Gold (Updated) on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(6d70)_Spooky_Scary_Skeletons_-_Andrew_Gold_(Updated)\\\\ExpertPlus.npy'\n",
      "Looping over 227 batches\n",
      "Time taken for song: 26.33 seconds\n",
      "Saved_Features\\v1_(60f4)_The_Alien_-_Noisecream\\ExpertPlus.npy\n",
      "Calculating features and placements for The Alien - Noisecream on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(60f4)_The_Alien_-_Noisecream\\\\ExpertPlus.npy'\n",
      "Looping over 275 batches\n",
      "Time taken for song: 33.42 seconds\n",
      "Saved_Features\\v1_(f407)_Waving_Through_a_Window_-_Dear_Evan_Hansen\\Expert.npy\n",
      "Calculating features and placements for Waving Through a Window - Dear Evan Hansen on Expert difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(f407)_Waving_Through_a_Window_-_Dear_Evan_Hansen\\\\Expert.npy'\n",
      "Looping over 529 batches\n",
      "Time taken for song: 55.33 seconds\n",
      "Saved_Features\\v1_(411e)_The_Riddle_(Prezioso__Marvin)\\ExpertPlus.npy\n",
      "Calculating features and placements for The Riddle (Prezioso & Marvin) on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(411e)_The_Riddle_(Prezioso__Marvin)\\\\ExpertPlus.npy'\n",
      "Looping over 659 batches\n",
      "Time taken for song: 71.25 seconds\n",
      "Saved_Features\\v1_(1010f)_Jump_Up_Super_Star_(feat\\Expert.npy\n",
      "Calculating features and placements for Jump Up, Super Star! (feat. Jenny) on Expert difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(1010f)_Jump_Up_Super_Star_(feat\\\\Expert.npy'\n",
      "Looping over 1824 batches\n",
      "Time taken for song: 134.03 seconds\n",
      "Saved_Features\\v1_(aed7)_Khenab_-_Temple_Of_Sound\\Expert.npy\n",
      "Calculating features and placements for Khenab - Temple Of Sound on Expert difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(aed7)_Khenab_-_Temple_Of_Sound\\\\Expert.npy'\n",
      "Looping over 481 batches\n",
      "Time taken for song: 46.32 seconds\n",
      "Saved_Features\\v1_(24e3)_The_Other_Side_(The_Greatest_Showman)\\ExpertPlus.npy\n",
      "Calculating features and placements for The Other Side (The Greatest Showman) on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(24e3)_The_Other_Side_(The_Greatest_Showman)\\\\ExpertPlus.npy'\n",
      "Looping over 513 batches\n",
      "Time taken for song: 51.59 seconds\n",
      "Saved_Features\\v1_(2a8b)_Girls_Generation_()_-_The_Boys_(DJ_Huygens_Remix)\\Expert.npy\n",
      "Calculating features and placements for Girls' Generation (소녀시대) - The Boys (DJ Huygens Remix) on Expert difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(2a8b)_Girls_Generation_()_-_The_Boys_(DJ_Huygens_Remix)\\\\Expert.npy'\n",
      "Looping over 376 batches\n",
      "Time taken for song: 41.00 seconds\n",
      "Saved_Features\\v1_(9aa5)_Mercy_(feat\\ExpertPlus.npy\n",
      "Calculating features and placements for Mercy (feat. glasscat) - MitiS on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(9aa5)_Mercy_(feat\\\\ExpertPlus.npy'\n",
      "Looping over 511 batches\n",
      "Time taken for song: 52.76 seconds\n",
      "Saved_Features\\v1_(6e4f)_Give_A_Reason_(Full_ver)\\ExpertPlus.npy\n",
      "Calculating features and placements for Give A Reason (Full ver) on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(6e4f)_Give_A_Reason_(Full_ver)\\\\ExpertPlus.npy'\n",
      "Looping over 581 batches\n",
      "Time taken for song: 60.70 seconds\n",
      "Saved_Features\\v1_(54de)_Macky_Gee_Ft\\ExpertPlus.npy\n",
      "Calculating features and placements for Macky Gee Ft. Stuart Rowe - Aftershock  on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(54de)_Macky_Gee_Ft\\\\ExpertPlus.npy'\n",
      "Looping over 632 batches\n",
      "Time taken for song: 61.02 seconds\n",
      "Saved_Features\\v1_(3dfc)_Crime_and_Punishment_DECO27\\ExpertPlus.npy\n",
      "Calculating features and placements for Crime and Punishment DECO*27 on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(3dfc)_Crime_and_Punishment_DECO27\\\\ExpertPlus.npy'\n",
      "Looping over 325 batches\n",
      "Time taken for song: 37.39 seconds\n",
      "Saved_Features\\v1_(1052e)_Miley_Cyrus_-_Midnight_Sky\\Expert.npy\n",
      "Calculating features and placements for Miley Cyrus - Midnight Sky on Expert difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(1052e)_Miley_Cyrus_-_Midnight_Sky\\\\Expert.npy'\n",
      "Looping over 372 batches\n",
      "Time taken for song: 43.78 seconds\n",
      "Saved_Features\\v1_(148f8)_Zomboy_-_Gorilla_March\\ExpertPlus.npy\n",
      "Calculating features and placements for Zomboy - Gorilla March on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(148f8)_Zomboy_-_Gorilla_March\\\\ExpertPlus.npy'\n",
      "Looping over 648 batches\n",
      "Time taken for song: 65.38 seconds\n",
      "Saved_Features\\v1_(90f4)_NUCLEAR-STAR_-_Camellia\\ExpertPlus.npy\n",
      "Calculating features and placements for NUCLEAR-STAR - Camellia on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(90f4)_NUCLEAR-STAR_-_Camellia\\\\ExpertPlus.npy'\n",
      "Looping over 915 batches\n",
      "Time taken for song: 84.68 seconds\n",
      "Saved_Features\\v1_(ee66)_Windia_(Game_Size)_Sword_Art_Online_Hollow_Realization_Opening_-_Luna_Haruna\\ExpertPlus.npy\n",
      "Calculating features and placements for Windia (Game Size) [Sword Art Online: Hollow Realization Opening] - Luna Haruna on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(ee66)_Windia_(Game_Size)_Sword_Art_Online_Hollow_Realization_Opening_-_Luna_Haruna\\\\ExpertPlus.npy'\n",
      "Looping over 231 batches\n",
      "Time taken for song: 23.69 seconds\n",
      "Saved_Features\\v1_(e259)_Round_Wave_Crusher__Drugs\\ExpertPlus.npy\n",
      "Calculating features and placements for Round Wave Crusher | Drugs on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(e259)_Round_Wave_Crusher__Drugs\\\\ExpertPlus.npy'\n",
      "Looping over 755 batches\n",
      "Time taken for song: 72.07 seconds\n",
      "Saved_Features\\v1_(2df4)_MAYDAY_-_TheFatRat_(ft\\ExpertPlus.npy\n",
      "Calculating features and placements for MAYDAY - TheFatRat (ft. Laura Brehm) on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(2df4)_MAYDAY_-_TheFatRat_(ft\\\\ExpertPlus.npy'\n",
      "Looping over 571 batches\n",
      "Time taken for song: 58.64 seconds\n",
      "Saved_Features\\v1_(13bb4)_DRAGON_FORCE_-_Unlucky_Morpheus\\Expert.npy\n",
      "Calculating features and placements for DRAGON FORCE - Unlucky Morpheus on Expert difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(13bb4)_DRAGON_FORCE_-_Unlucky_Morpheus\\\\Expert.npy'\n",
      "Looping over 718 batches\n",
      "Time taken for song: 70.61 seconds\n",
      "Saved_Features\\v1_(4ac0)_Body_-_Loud_Luxury_feat\\Expert.npy\n",
      "Calculating features and placements for Body - Loud Luxury feat. brando on Expert difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(4ac0)_Body_-_Loud_Luxury_feat\\\\Expert.npy'\n",
      "Looping over 299 batches\n",
      "Time taken for song: 34.51 seconds\n",
      "Saved_Features\\v1_(8614)_Rewriter-P\\ExpertPlus.npy\n",
      "Calculating features and placements for Rewriter-八王子P on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(8614)_Rewriter-P\\\\ExpertPlus.npy'\n",
      "Looping over 103 batches\n",
      "Time taken for song: 15.87 seconds\n",
      "Saved_Features\\v1_(5c78)_Walk_Like_an_Egyptian\\Expert.npy\n",
      "Calculating features and placements for Walk Like an Egyptian on Expert difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(5c78)_Walk_Like_an_Egyptian\\\\Expert.npy'\n",
      "Looping over 315 batches\n",
      "Time taken for song: 38.48 seconds\n",
      "Saved_Features\\v1_(2c2e)_Follow_Me_-_TryHardNinja\\ExpertPlus.npy\n",
      "Calculating features and placements for Follow Me - TryHardNinja on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(2c2e)_Follow_Me_-_TryHardNinja\\\\ExpertPlus.npy'\n",
      "Looping over 400 batches\n",
      "Time taken for song: 44.12 seconds\n",
      "Saved_Features\\v1_(706f)_abababababababa\\ExpertPlus.npy\n",
      "Calculating features and placements for abababababababa on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(706f)_abababababababa\\\\ExpertPlus.npy'\n",
      "Looping over 38 batches\n",
      "Time taken for song: 6.92 seconds\n",
      "Saved_Features\\v1_(131da)_DJ_Okawari_-_Flower_Dance_V1_(With_One_Saber_from_2018)\\ExpertPlus.npy\n",
      "Calculating features and placements for DJ Okawari - Flower Dance V1 (With One Saber from 2018) on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(131da)_DJ_Okawari_-_Flower_Dance_V1_(With_One_Saber_from_2018)\\\\ExpertPlus.npy'\n",
      "Looping over 362 batches\n",
      "Time taken for song: 45.77 seconds\n",
      "Saved_Features\\v1_(699)_DJ_Hyper_-_Spoiler\\Expert.npy\n",
      "Calculating features and placements for DJ Hyper - Spoiler on Expert difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(699)_DJ_Hyper_-_Spoiler\\\\Expert.npy'\n",
      "Looping over 372 batches\n",
      "Time taken for song: 53.65 seconds\n",
      "Saved_Features\\v1_(11fa3)_nonoc_-_Memento_(TV_Size)_(ReZero__Re_Zero_S2P1_ED)\\Expert.npy\n",
      "Calculating features and placements for nonoc - Memento (TV Size) (Re:Zero / Re Zero S2P1 ED) on Expert difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(11fa3)_nonoc_-_Memento_(TV_Size)_(ReZero__Re_Zero_S2P1_ED)\\\\Expert.npy'\n",
      "Looping over 149 batches\n",
      "Time taken for song: 18.93 seconds\n",
      "Saved_Features\\v1_(6471)_Fries\\Expert.npy\n",
      "Calculating features and placements for Fries on Expert difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(6471)_Fries\\\\Expert.npy'\n",
      "Looping over 296 batches\n",
      "Time taken for song: 31.34 seconds\n",
      "Saved_Features\\v1_(8d59)_Salvatore_Ganacci__Megatone_-_Cake\\Expert.npy\n",
      "Calculating features and placements for Salvatore Ganacci & Megatone - Cake on Expert difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(8d59)_Salvatore_Ganacci__Megatone_-_Cake\\\\Expert.npy'\n",
      "Looping over 333 batches\n",
      "Time taken for song: 35.68 seconds\n",
      "Saved_Features\\v1_(5f8c)_Hot_Pink_-_EXID\\Expert.npy\n",
      "Calculating features and placements for Hot Pink - EXID on Expert difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(5f8c)_Hot_Pink_-_EXID\\\\Expert.npy'\n",
      "Looping over 323 batches\n",
      "Time taken for song: 39.87 seconds\n",
      "Saved_Features\\v1_(11b90)_Manipulate\\ExpertPlus.npy\n",
      "Calculating features and placements for Manipulate on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(11b90)_Manipulate\\\\ExpertPlus.npy'\n",
      "Looping over 355 batches\n",
      "Time taken for song: 41.57 seconds\n",
      "Saved_Features\\v1_(5f4b)_Off_with_His_Shirt_Galavant_Season_2_Soundtrack_-_Cast_of_Galavant\\Expert.npy\n",
      "Calculating features and placements for Off with His Shirt [Galavant Season 2 Soundtrack] - Cast of Galavant on Expert difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(5f4b)_Off_with_His_Shirt_Galavant_Season_2_Soundtrack_-_Cast_of_Galavant\\\\Expert.npy'\n",
      "Looping over 264 batches\n",
      "Time taken for song: 29.56 seconds\n",
      "Saved_Features\\v1_(13133)_Here_Without_You_-_3_Doors_Down\\ExpertPlus.npy\n",
      "Calculating features and placements for Here Without You - 3 Doors Down on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(13133)_Here_Without_You_-_3_Doors_Down\\\\ExpertPlus.npy'\n",
      "Looping over 529 batches\n",
      "Time taken for song: 54.70 seconds\n",
      "Saved_Features\\v1_(58f0)_PA_PA_YA_-_BABYMETAL_ft\\ExpertPlus.npy\n",
      "Calculating features and placements for PA PA YA!! - BABYMETAL ft. F.HERO on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(58f0)_PA_PA_YA_-_BABYMETAL_ft\\\\ExpertPlus.npy'\n",
      "Looping over 587 batches\n",
      "Time taken for song: 58.24 seconds\n",
      "Saved_Features\\v1_(3237)_Aint_Got_Rhythm_-_Phineas_and_Ferb\\Expert.npy\n",
      "Calculating features and placements for Ain't Got Rhythm - Phineas and Ferb on Expert difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(3237)_Aint_Got_Rhythm_-_Phineas_and_Ferb\\\\Expert.npy'\n",
      "Looping over 484 batches\n",
      "Time taken for song: 44.70 seconds\n",
      "Saved_Features\\v1_(6c11)_Laur_-_Exitium\\ExpertPlus.npy\n",
      "Calculating features and placements for Laur - Exitium on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(6c11)_Laur_-_Exitium\\\\ExpertPlus.npy'\n",
      "Looping over 564 batches\n",
      "Time taken for song: 49.21 seconds\n",
      "Saved_Features\\v1_(649e)_Electro_Swing_Odd_Chap_-_Top_Secret\\ExpertPlus.npy\n",
      "Calculating features and placements for [Electro Swing] Odd Chap - Top Secret on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(649e)_Electro_Swing_Odd_Chap_-_Top_Secret\\\\ExpertPlus.npy'\n",
      "Looping over 437 batches\n",
      "Time taken for song: 48.84 seconds\n",
      "Saved_Features\\v1_(a203)_Ranked_Tinnitus_(Schwanks_PTSD_Remix)\\ExpertPlus.npy\n",
      "Calculating features and placements for [Ranked] Tinnitus (Schwank's PTSD Remix) on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(a203)_Ranked_Tinnitus_(Schwanks_PTSD_Remix)\\\\ExpertPlus.npy'\n",
      "Looping over 601 batches\n",
      "Time taken for song: 56.00 seconds\n",
      "Saved_Features\\v1_(4bb1)_Pushing_Onwards_-_Magnus_Palsson_(VVVVVV_Soundtrack)\\Expert.npy\n",
      "Calculating features and placements for Pushing Onwards - Magnus Palsson (VVVVVV Soundtrack) on Expert difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(4bb1)_Pushing_Onwards_-_Magnus_Palsson_(VVVVVV_Soundtrack)\\\\Expert.npy'\n",
      "Looping over 481 batches\n",
      "Time taken for song: 51.59 seconds\n",
      "Saved_Features\\v1_(2512)_Carol_of_the_Bells_Instrumental_by_Everfound\\Expert.npy\n",
      "Calculating features and placements for Carol of the Bells Instrumental by Everfound on Expert difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(2512)_Carol_of_the_Bells_Instrumental_by_Everfound\\\\Expert.npy'\n",
      "Looping over 552 batches\n",
      "Time taken for song: 54.37 seconds\n",
      "Saved_Features\\v1_(9c6d)_Kylie_Minogue_-_All_The_Lovers\\Expert.npy\n",
      "Calculating features and placements for Kylie Minogue - All The Lovers on Expert difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(9c6d)_Kylie_Minogue_-_All_The_Lovers\\\\Expert.npy'\n",
      "Looping over 724 batches\n",
      "Time taken for song: 77.25 seconds\n",
      "Saved_Features\\v1_(66c4)_Inferno_-_Mrs\\ExpertPlus.npy\n",
      "Calculating features and placements for Inferno - Mrs. Green Apple on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(66c4)_Inferno_-_Mrs\\\\ExpertPlus.npy'\n",
      "Looping over 619 batches\n",
      "Time taken for song: 57.76 seconds\n",
      "Saved_Features\\v1_(13292)_Camellia_-_Shun_no_Shifudo\\ExpertPlus.npy\n",
      "Calculating features and placements for Camellia - Shun no Shifudo on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(13292)_Camellia_-_Shun_no_Shifudo\\\\ExpertPlus.npy'\n",
      "Looping over 965 batches\n",
      "Time taken for song: 87.96 seconds\n",
      "Saved_Features\\v1_(7185)_Extra_Magic_Hour_(TV_Size)_Amagi_Brilliant_Park_Opening_-_AKINO_with_bless4\\ExpertPlus.npy\n",
      "Calculating features and placements for Extra Magic Hour (TV Size) [Amagi Brilliant Park Opening] - AKINO with bless4 on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(7185)_Extra_Magic_Hour_(TV_Size)_Amagi_Brilliant_Park_Opening_-_AKINO_with_bless4\\\\ExpertPlus.npy'\n",
      "Looping over 235 batches\n",
      "Time taken for song: 26.09 seconds\n",
      "Saved_Features\\v1_(7415)_Magnolia\\ExpertPlus.npy\n",
      "Calculating features and placements for Magnolia on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(7415)_Magnolia\\\\ExpertPlus.npy'\n",
      "Looping over 378 batches\n",
      "Time taken for song: 41.46 seconds\n",
      "Saved_Features\\v1_(d2)_DM_Ashura_-_deltaMAX\\ExpertPlus.npy\n",
      "Calculating features and placements for DM Ashura - deltaMAX on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(d2)_DM_Ashura_-_deltaMAX\\\\ExpertPlus.npy'\n",
      "Looping over 160 batches\n",
      "Time taken for song: 23.86 seconds\n",
      "Saved_Features\\v1_(13a38)_PLight_-_ADAM\\Expert.npy\n",
      "Calculating features and placements for P*Light - ADAM on Expert difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(13a38)_PLight_-_ADAM\\\\Expert.npy'\n",
      "Looping over 360 batches\n",
      "Time taken for song: 38.11 seconds\n",
      "Saved_Features\\v1_(20ed)_Lick_It_-_Valentino_Khan\\Expert.npy\n",
      "Calculating features and placements for Lick It - Valentino Khan on Expert difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(20ed)_Lick_It_-_Valentino_Khan\\\\Expert.npy'\n",
      "Looping over 277 batches\n",
      "Time taken for song: 34.73 seconds\n",
      "Saved_Features\\v1_(4a1d)_Nai_Br\\Expert.npy\n",
      "Calculating features and placements for Nai Br.XX & Celeina Ann - Hold Me Now [Carole and Tuesday Ending] on Expert difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(4a1d)_Nai_Br\\\\Expert.npy'\n",
      "Looping over 152 batches\n",
      "Time taken for song: 19.99 seconds\n",
      "Saved_Features\\v1_(8c0a)_BLACK_LABEL\\ExpertPlus.npy\n",
      "Calculating features and placements for BLACK LABEL on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(8c0a)_BLACK_LABEL\\\\ExpertPlus.npy'\n",
      "Looping over 522 batches\n",
      "Time taken for song: 55.56 seconds\n",
      "Saved_Features\\v1_(4f2e)_Final_Letter\\Expert.npy\n",
      "Calculating features and placements for Final Letter on Expert difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(4f2e)_Final_Letter\\\\Expert.npy'\n",
      "Looping over 378 batches\n",
      "Time taken for song: 35.76 seconds\n",
      "Saved_Features\\v1_(abd5)_PSYQUI_feat\\Expert.npy\n",
      "Calculating features and placements for PSYQUI feat. Marpril - Girly Cupid on Expert difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(abd5)_PSYQUI_feat\\\\Expert.npy'\n",
      "Looping over 499 batches\n",
      "Time taken for song: 54.49 seconds\n",
      "Saved_Features\\v1_(5af1)_Oresama_-_Meteor_Dance_Floor_\\ExpertPlus.npy\n",
      "Calculating features and placements for Oresama - Meteor Dance Floor  on ExpertPlus difficulty. Exception: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(5af1)_Oresama_-_Meteor_Dance_Floor_\\\\ExpertPlus.npy'\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-09060dcb9cdf>\u001b[0m in \u001b[0;36mtrain_network\u001b[1;34m(model, train_df, val_df, num_epochs, learning_rate, batch_size)\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m                 \u001b[0mfeatures_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Successfully opened file for {} on {} difficulty\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeatmap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msong_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Saved_Features\\\\v1_(5af1)_Oresama_-_Meteor_Dance_Floor_\\\\ExpertPlus.npy'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-2cbad43a2665>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtrain_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbm_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-92-09060dcb9cdf>\u001b[0m in \u001b[0;36mtrain_network\u001b[1;34m(model, train_df, val_df, num_epochs, learning_rate, batch_size)\u001b[0m\n\u001b[0;32m     35\u001b[0m                     \u001b[0msong_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'(^.+\\.(egg|ogg|mp4|mp3))'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mI\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                     \u001b[0mfolder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msong_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m                     features, padded_placements = get_features(song_path, bpm=beatmap.bpm, placements=placements, \n\u001b[0m\u001b[0;32m     38\u001b[0m                                                                save_data=True, difficulty=diff, folder_path=beatmap.file_path)\n\u001b[0;32m     39\u001b[0m                     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msong_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-89-d268f4e56fcc>\u001b[0m in \u001b[0;36mget_features\u001b[1;34m(song_path, bpm, placements, save_data, difficulty, version, folder_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msong_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbpm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplacements\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdifficulty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'expertPlus'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeatures_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfolder_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mbeat_frames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmelspectrogram\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchromagram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_audio_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msong_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbpm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mquarter_frames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Array to store data for every quarter note\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeat_frames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-c2f9e764a196>\u001b[0m in \u001b[0;36mget_audio_data\u001b[1;34m(song_path, bpm)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msong_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0msr\u001b[0m \u001b[1;31m# Song length according to librosa in secs (doesn't match given length for some reason)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0my_harmonic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_percussive\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meffects\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhpss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mtempo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeat_frames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeat_track\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbpm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbpm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# Reduce n_mels to avoid empty filters in mel frequency basis and to match the size of chrormagram\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\librosa\\effects.py\u001b[0m in \u001b[0;36mhpss\u001b[1;34m(y, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;31m# Decompose into harmonic and percussives\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[0mstft_harm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstft_perc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecompose\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhpss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;31m# Invert the STFTs.  Adjust length to match the input.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\librosa\\decompose.py\u001b[0m in \u001b[0;36mhpss\u001b[1;34m(S, kernel_size, power, mask, margin)\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;31m# Compute median filters. Pre-allocation here preserves memory layout.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[0mharm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m     \u001b[0mharm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmedian_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwin_harm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"reflect\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[0mperc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\ndimage\\filters.py\u001b[0m in \u001b[0;36mmedian_filter\u001b[1;34m(input, size, footprint, output, mode, cval, origin)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;33m>>\u001b[0m\u001b[1;33m>\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m     \"\"\"\n\u001b[1;32m-> 1321\u001b[1;33m     return _rank_filter(input, 0, size, footprint, output, mode, cval,\n\u001b[0m\u001b[0;32m   1322\u001b[0m                         origin, 'median')\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\ndimage\\filters.py\u001b[0m in \u001b[0;36m_rank_filter\u001b[1;34m(input, rank, size, footprint, output, mode, cval, origin, operation)\u001b[0m\n\u001b[0;32m   1236\u001b[0m                 \"filters\")\n\u001b[0;32m   1237\u001b[0m         \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ni_support\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_mode_to_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1238\u001b[1;33m         _nd_image.rank_filter(input, rank, footprint, output, mode, cval,\n\u001b[0m\u001b[0;32m   1239\u001b[0m                               origins)\n\u001b[0;32m   1240\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtemp_needed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#==================================== Training Settings ===================================#\n",
    "learning_rate = 0.004   # Learning rate\n",
    "num_epochs = 5          # Number of epochs\n",
    "batch_size = 32         # Number of sequences to batch together\n",
    "num_workers = 1         # Number of workers to load the data\n",
    "features_version = 1    # If I make any large changes that require re-calculating features\n",
    "#==========================================================================================#\n",
    "\n",
    "\n",
    "train_network(bm_gen, train_df, val_df, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(song_path, bpm, placements=None, save_data=False, difficulty='expertPlus', version=features_version, folder_path=''):\n",
    "    beat_frames, sr, melspectrogram, chromagram = get_audio_data(song_path, bpm)\n",
    "    quarter_frames = np.array([]) # Array to store data for every quarter note\n",
    "    index = 0\n",
    "    while index < len(beat_frames) - 1:\n",
    "        quarter_frames = np.append(quarter_frames, np.arange(beat_frames[index], beat_frames[index + 1], \n",
    "                                                            round((beat_frames[index + 1] - beat_frames[index]) / 4))[1:4])\n",
    "        index += 1\n",
    "    \n",
    "    beat_frames_with_quarter = np.concatenate((np.array([0]), beat_frames, quarter_frames), axis=0)\n",
    "    beat_frames_with_quarter.sort() # We appended the quarter notes so we need to sort them into right spots\n",
    "    beat_frames_with_quarter = beat_frames_with_quarter.astype(int)\n",
    "\n",
    "    beat_frames_with_mel = librosa.util.sync(melspectrogram, beat_frames_with_quarter, aggregate=np.median)\n",
    "    beat_frames_with_chroma = librosa.util.sync(chromagram, beat_frames_with_quarter, aggregate=np.median)\n",
    "\n",
    "    # Want to extend it to 1/16 beats as 1/4 beats limits the max NPS by a lot\n",
    "    # Using pandas to easily extend to 1/16 beats\n",
    "    beat_num_with_16th_notes = np.arange(0, len(beat_frames) - 1, 1.0 / 16.0)\n",
    "    beat_num = np.arange(0, len(beat_frames), 1)\n",
    "    beats_num_df = pd.concat([pd.Series(beat_num, name='beat_num', dtype=int), pd.Series(beat_frames, name='beat_frame', dtype=int)], axis=1)\n",
    "    mel_beats_df = pd.concat([pd.Series(beat_frames_with_quarter, name='quater_beat_frame'), pd.DataFrame(beat_frames_with_mel.T)], axis=1)\n",
    "    chroma_beats_df = pd.concat([pd.Series(beat_frames_with_quarter, name='quater_beat_frame'), pd.DataFrame(beat_frames_with_chroma.T)], axis=1)\n",
    "\n",
    "    # Dataframe with the beat number, frame number, mel data, and chroma data for the beat\n",
    "    beats_df = beats_num_df.merge(mel_beats_df, how='outer', left_on='beat_frame', right_on='quater_beat_frame', sort=True)\n",
    "\n",
    "    # Only need the quarter beat frames\n",
    "    beats_df = beats_df.drop(columns=['beat_frame']) \n",
    "    beats_df = beats_df.merge(chroma_beats_df, how='outer', on='quater_beat_frame', sort=True)\n",
    "\n",
    "    # Removes the NaNs from the beat number column for next merge\n",
    "    beats_df.interpolate(inplace=True) \n",
    "\n",
    "    # Expand it to be 1/16 beats. Doing it in 1/4 beats saves a lot of computation with sync\n",
    "    beats_num_16th_notes_df = pd.DataFrame(beat_num_with_16th_notes, columns=['beat_num'])\n",
    "    beats_df = beats_df.merge(beats_num_16th_notes_df, how='outer', on='beat_num', sort=True)\n",
    "\n",
    "    # Interpolate the quarter beat frame so we can more accuractly place the placements\n",
    "    beats_df['quater_beat_frame'].interpolate(inplace=True) \n",
    "    beats_df['quater_beat_frame'] = beats_df['quater_beat_frame'].round()\n",
    "    beats_df = beats_df.fillna(method='pad') # Forward fill\n",
    "\n",
    "    # Add placement column to store what note type is at that time\n",
    "    beats_df.insert(1, 'placement', 0)\n",
    "    # Add column for time since last note since this helps dicate what placements should be done\n",
    "    beats_df.insert(3, 'time_since_last_note', 0.0)\n",
    "\n",
    "    # Now the computed audio values for each quarter note are spread among 1/16 notes so we can use sequences of 1/16 notes\n",
    "    for timing, placement in placements.items():\n",
    "        time_in_frames = librosa.core.time_to_frames(beat_to_time(timing, bpm), sr=sr)\n",
    "        try:\n",
    "            matching_frames = beats_df.loc[beats_df['quater_beat_frame'] == time_in_frames, 'placement']\n",
    "            if len(matching_frames) == 1:\n",
    "                beats_df.loc[beats_df['quater_beat_frame'] == time_in_frames, 'placement'] = placement\n",
    "            else: # Must be more than one quater beat on that frame\n",
    "                placement_arr = [placement]\n",
    "                placement_arr.extend([0] * (len(matching_frames) - 1))\n",
    "                beats_df.loc[beats_df['quater_beat_frame'] == time_in_frames, 'placement'] = placement_arr\n",
    "        except: # No exact note timing match\n",
    "            # Subtract the value and find the one closest to 0\n",
    "            closest_index = beats_df['quater_beat_frame'].sub(time_in_frames).abs().idxmin()\n",
    "            beats_df[closest_index]['placement'] = placement\n",
    "    # Set the placements with 16 to 0 as they represent the same board but are easier to distinguish\n",
    "    beats_df.loc[beats_df['placement'] == 16, 'placement'] = 0\n",
    "\n",
    "    # Extract from pandas into list which we can turn into tensor later\n",
    "    features = []\n",
    "    last_note_time = 0\n",
    "    for i in range(len(beats_df.index)):\n",
    "        # Set the time since the last note\n",
    "        time_in_s = librosa.core.frames_to_time(beats_df.at[i, 'quater_beat_frame'], sr=sr)\n",
    "        if beats_df.at[i, 'placement'] != 0 and beats_df.at[i, 'placement'] != 16:\n",
    "            last_note_time = time_in_s\n",
    "        if last_note_time != 0:\n",
    "            beats_df.at[i, 'time_since_last_note'] = (time_in_s - last_note_time)\n",
    "        # Extract the data from this row\n",
    "        features.append(beats_df.iloc[i, 3:].tolist())\n",
    "\n",
    "    # Convert to numpy arrays for easier slicing later\n",
    "    features = np.asarray(features)\n",
    "    # Placements made by the human mapper\n",
    "    human_placements = beats_df['placement'].to_numpy()\n",
    "\n",
    "    if save_data:\n",
    "        feat_dict = {'features' : features, 'placements' : human_placements}\n",
    "        folder_name = folder_path.rsplit('/', 1)[-1]\n",
    "        folder_name = folder_name.split('.')[0]\n",
    "        path = 'Saved_Features/v{}_{}/'.format(version, folder_name)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        np.save(os.path.join(path, '{}.npy'.format(difficulty)), feat_dict)\n",
    "\n",
    "    return features, human_placements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_data(song_path, bpm):\n",
    "    y, sr = librosa.load(song_path)\n",
    "    length = y.shape[0] / sr # Song length according to librosa in secs (doesn't match given length for some reason)\n",
    "    y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
    "    tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr, trim=False, bpm=bpm)\n",
    "    # Reduce n_mels to avoid empty filters in mel frequency basis and to match the size of chrormagram\n",
    "    melspectrogram = librosa.feature.melspectrogram(y=y_percussive, sr=sr, n_mels=12, fmax=65.4)\n",
    "    chromagram = librosa.feature.chroma_cqt(y=y_harmonic, sr=sr)\n",
    "    return beat_frames, sr, melspectrogram, chromagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sequences(features, placements, seq_len):\n",
    "    feature_sequences = []\n",
    "    placements_for_seq = []\n",
    "    for i in range(len(features)):\n",
    "        end_index = i + seq_len\n",
    "        if end_index > len(features):\n",
    "            break\n",
    "        feature_seq = features[i:end_index, :] \n",
    "        placement_for_seq = placements[i]\n",
    "        feature_sequences.append(feature_seq)\n",
    "        placements_for_seq.append(placement_for_seq)\n",
    "    return np.array(feature_sequences), np.array(placements_for_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(net, data):\n",
    "    correct, total = 0, 0\n",
    "    for sms, labels in data:\n",
    "        output = net(sms[0])\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        total += labels.shape[0]\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to do some conversions\n",
    "# Time in seconds to beat number\n",
    "def time_to_beat(note_time, bpm):\n",
    "    return (note_time / 60) * bpm\n",
    "\n",
    "# Beat number to seconds\n",
    "def beat_to_time(beat_time, bpm):\n",
    "    return (beat_time / bpm) * 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns file path to folder containing all files needed to play song made by model\n",
    "def get_map_from_song(song_path, model, seq_len=512, output_file_path='Expert.dat', start_time=2, bpm=0):\n",
    "    # Get the onset times where we will place notes\n",
    "    onset_times = get_onset_times(song_path, min_sep=0.1)\n",
    "    num_before = len(onset_times)\n",
    "    onset_times = np.delete(onset_times, np.where(onset_times <= start_time))\n",
    "    print(\"Removed {} onset times for being before the specified start time\".format(num_before - len(onset_times)))\n",
    "    # If the bpm is not provided then we calculate it ourselves\n",
    "    if bpm == 0:\n",
    "        y, samp_rate = librosa.load(song_file)\n",
    "        bpm = librosa.beat.tempo(y=y, sr=samp_rate)\n",
    "        print(\"Got a bpm of {}\".format(bpm))\n",
    "    # Determine the notes we should place\n",
    "    beats_df, sr = generate_placements(song_path, model, bpm, onset_times, seq_len)\n",
    "\n",
    "    notes_as_json = convert_model_placements_to_valid_json(beats_df, most_common_placements, sr, bpm)\n",
    "    with open(output_file_path, 'w') as dat_file:\n",
    "        dat_data = {\"_version\": \"2.2.0\",\n",
    "                    \"_customData\": {\n",
    "                        \"_time\": '',\n",
    "                        \"_BPMChanges\": [],\n",
    "                        \"_bookmarks\": []\n",
    "                        },\n",
    "                    \"_events\": [],\n",
    "                    \"_notes\": notes_as_json,\n",
    "                    \"_obstacles\": [],\n",
    "                    \"_waypoints\": []\n",
    "                    }\n",
    "        json.dump(dat_data, dat_file)\n",
    "    \n",
    "    print(\"Number of notes placed: {}\\nNumber of unique note placements: {}\\nApprox. notes per second: {}\".format(\n",
    "            len(notes_as_json),\n",
    "            len(set(notes_list)),\n",
    "            len(notes_as_json) / np.amax(onset_times)\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Removed 12 onset times for being within 0.1s of the next note\n",
      "Removed 1 onset times for being before the specified start time\n",
      "At timing\n",
      "torch.Size([1, 2001])\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "argmax(): argument 'dim' (position 1) must be int, not Tensor",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-151-92a07e1c1956>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_map_from_song\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'redo.egg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbm_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseq_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbpm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m190\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-137-ac97d14c4056>\u001b[0m in \u001b[0;36mget_map_from_song\u001b[1;34m(song_path, model, seq_len, output_file_path, start_time, bpm)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Got a bpm of {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbpm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# Determine the notes we should place\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mbeats_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_placements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msong_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbpm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monset_times\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mnotes_as_json\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_model_placements_to_valid_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeats_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmost_common_placements\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbpm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-150-c109ad787a7a>\u001b[0m in \u001b[0;36mgenerate_placements\u001b[1;34m(song_path, model, bpm, onset_times, seq_len, difficulty, version)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mchosen_placement\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m         \u001b[0mbeats_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'placement'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchosen_placement\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: argmax(): argument 'dim' (position 1) must be int, not Tensor"
     ]
    }
   ],
   "source": [
    "get_map_from_song('redo.egg', bm_gen, seq_len=seq_size, bpm=190)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_placements(song_path, model, bpm, onset_times, seq_len, difficulty='expertPlus', version=features_version):\n",
    "    beat_frames, sr, melspectrogram, chromagram = get_audio_data(song_path, bpm)\n",
    "    quarter_frames = np.array([]) # Array to store data for every quarter note\n",
    "    index = 0\n",
    "    while index < len(beat_frames) - 1:\n",
    "        quarter_frames = np.append(quarter_frames, np.arange(beat_frames[index], beat_frames[index + 1], \n",
    "                                                            round((beat_frames[index + 1] - beat_frames[index]) / 4))[1:4])\n",
    "        index += 1\n",
    "    \n",
    "    beat_frames_with_quarter = np.concatenate((np.array([0]), beat_frames, quarter_frames), axis=0)\n",
    "    beat_frames_with_quarter.sort() # We appended the quarter notes so we need to sort them into right spots\n",
    "    beat_frames_with_quarter = beat_frames_with_quarter.astype(int)\n",
    "\n",
    "    beat_frames_with_mel = librosa.util.sync(melspectrogram, beat_frames_with_quarter, aggregate=np.median)\n",
    "    beat_frames_with_chroma = librosa.util.sync(chromagram, beat_frames_with_quarter, aggregate=np.median)\n",
    "\n",
    "    # Want to extend it to 1/16 beats as 1/4 beats limits the max NPS by a lot\n",
    "    # Using pandas to \"easily\" extend to 1/16 beats\n",
    "    beat_num_with_16th_notes = np.arange(0, len(beat_frames) - 1, 1.0 / 16.0)\n",
    "    beat_num = np.arange(0, len(beat_frames), 1)\n",
    "    beats_num_df = pd.concat([pd.Series(beat_num, name='beat_num', dtype=int), pd.Series(beat_frames, name='beat_frame', dtype=int)], axis=1)\n",
    "    mel_beats_df = pd.concat([pd.Series(beat_frames_with_quarter, name='quater_beat_frame'), pd.DataFrame(beat_frames_with_mel.T)], axis=1)\n",
    "    chroma_beats_df = pd.concat([pd.Series(beat_frames_with_quarter, name='quater_beat_frame'), pd.DataFrame(beat_frames_with_chroma.T)], axis=1)\n",
    "\n",
    "    # Dataframe with the beat number, frame number, mel data, and chroma data for the beat\n",
    "    beats_df = beats_num_df.merge(mel_beats_df, how='outer', left_on='beat_frame', right_on='quater_beat_frame', sort=True)\n",
    "\n",
    "    # Only need the quarter beat frames\n",
    "    beats_df = beats_df.drop(columns=['beat_frame']) \n",
    "    beats_df = beats_df.merge(chroma_beats_df, how='outer', on='quater_beat_frame', sort=True)\n",
    "\n",
    "    # Removes the NaNs from the beat number column for next merge\n",
    "    beats_df.interpolate(inplace=True) \n",
    "\n",
    "    # Expand it to be 1/16 beats. Doing it in 1/4 beats saves a lot of computation with sync\n",
    "    beats_num_16th_notes_df = pd.DataFrame(beat_num_with_16th_notes, columns=['beat_num'])\n",
    "    beats_df = beats_df.merge(beats_num_16th_notes_df, how='outer', on='beat_num', sort=True)\n",
    "\n",
    "    # Interpolate the quarter beat frame so we can more accuractly place the placements\n",
    "    beats_df['quater_beat_frame'].interpolate(inplace=True) \n",
    "    beats_df['quater_beat_frame'] = beats_df['quater_beat_frame'].round()\n",
    "    beats_df = beats_df.fillna(method='pad') # Forward fill\n",
    "\n",
    "    # Add placement column to store what note type is at that time\n",
    "    beats_df.insert(1, 'placement', 0)\n",
    "    # Add column for time since last note since this helps dicate what placements should be done\n",
    "    beats_df.insert(3, 'time_since_last_note', 0.0)\n",
    "    print(\"At timing\")\n",
    "    for timing in onset_times:\n",
    "        time_in_frames = librosa.core.time_to_frames(beat_to_time(timing, bpm), sr=sr)\n",
    "        try:\n",
    "            matching_frames = beats_df.loc[beats_df['quater_beat_frame'] == time_in_frames, 'placement']\n",
    "            if len(matching_frames) == 1:\n",
    "                beats_df.loc[beats_df['quater_beat_frame'] == time_in_frames, 'placement'] = 9999\n",
    "            else: # Must be more than one quater beat on that frame\n",
    "                placement_arr = [9999]\n",
    "                placement_arr.extend([0] * (len(matching_frames) - 1))\n",
    "                beats_df.loc[beats_df['quater_beat_frame'] == time_in_frames, 'placement'] = placement_arr\n",
    "        except: # No exact note timing match\n",
    "            # Subtract the value and find the one closest to 0\n",
    "            closest_index = beats_df['quater_beat_frame'].sub(time_in_frames).abs().idxmin()\n",
    "            beats_df[closest_index]['placement'] = 9999            \n",
    "    \n",
    "    last_note_time = 0\n",
    "    for i in range(len(beats_df.index)):\n",
    "        # Set the time since the last note\n",
    "        time_in_s = librosa.core.frames_to_time(beats_df.at[i, 'quater_beat_frame'], sr=sr)\n",
    "        if beats_df.at[i, 'placement'] != 0 and beats_df.at[i, 'placement'] != 16:\n",
    "            last_note_time = time_in_s\n",
    "        if last_note_time != 0:\n",
    "            beats_df.at[i, 'time_since_last_note'] = (time_in_s - last_note_time)\n",
    "    \n",
    "    model.init_hidden_layer(batch_size=1)\n",
    "    placement_indicies = list(np.where(beats_df['placement'] == 9999)[0])\n",
    "    for index in placement_indicies:\n",
    "        seq = []\n",
    "        start_index = index - seq_len\n",
    "        # print(type(index))\n",
    "        # print(type(placement_indicies))\n",
    "        # print(type(start_index))\n",
    "        # print(start_index)\n",
    "        if start_index <= 0:\n",
    "            num_padding = abs(start_index)\n",
    "            seq.extend([[0] * len(beats_df.iloc[i, 3:].tolist())] * num_padding)\n",
    "            start_index = 0\n",
    "        relev_df = beats_df.iloc[start_index:index, 3:]\n",
    "        for ind, d in relev_df.iterrows():\n",
    "            seq.append(d.tolist())\n",
    "        # print(seq)\n",
    "        seq = np.array(seq)\n",
    "        feature_tensor = torch.from_numpy(seq).float()\n",
    "        if torch.cuda.is_available():\n",
    "            feature_tensor = feature_tensor.cuda()\n",
    "        output = model(feature_tensor.unsqueeze(0))\n",
    "        print(output.shape)\n",
    "        chosen_placement = torch.argmax(output, 1)\n",
    "        beats_df.at[index, 'placement'] = chosen_placement\n",
    "\n",
    "    return beats_df, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_model_placements_to_valid_json(beats_df, most_common_placements, sr, bpm):\n",
    "    list_of_jsons = []\n",
    "    for beat in beats_df.loc[beats_df['placement'] != 0].iterrows():\n",
    "        placement = beat['placement']\n",
    "        time_in_beat = time_to_beat(librosa.core.frames_to_time(beat['quater_beat_frame'], sr=sr), bpm)\n",
    "        placement_info = most_common_placements[placement]\n",
    "        for i in range(len(placement_info)):\n",
    "            if placement_info[i] != 0:\n",
    "                val = placement_info[i]\n",
    "                # 0 - Red, 1 - Blue \n",
    "                colour = 0 if val < 10 else 1\n",
    "                note_dir = val - (colour * 9) - 1\n",
    "                col = i % 4\n",
    "                row = (i - col) / 4\n",
    "                note_json = {\"_time\": time_in_beat,\n",
    "                            \"_lineIndex\": col,\n",
    "                            \"_lineLayer\": row,\n",
    "                            \"_type\": colour,\n",
    "                            \"_cutDirection\": note_dir}\n",
    "                list_of_jsons.append(note_json)\n",
    "    return list_of_jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(0, 0, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "print(most_common_placements[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Dict_Dataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, features, targets):\n",
    "#         self.targets = targets\n",
    "#         self.features = features\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         print('index : {}'.format(index))\n",
    "#         if index >= self.__len__():\n",
    "#             raise IndexError\n",
    "#         feature = self.features[index]\n",
    "#         target = self.targets[index]\n",
    "#         print('Index: {}. Got feature and target {}'.format(index, self.targets[index]))\n",
    "#         return {'feature' : feature, 'target' : target}\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([512, 512, 25])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_batch = feature_sequences[0:512,:,:]\n",
    "features_tensor = torch.tensor(feature_batch, dtype=torch.float32)\n",
    "if torch.cuda.is_available():\n",
    "    features_tensor = features_tensor.cuda()\n",
    "print(features_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.96 GiB (GPU 0; 8.00 GiB total capacity; 5.19 GiB already allocated; 873.07 MiB free; 5.33 GiB reserved in total by PyTorch)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-438-e472262bb6dd>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# get the next output and hidden state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;31m# predict distribution over next tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'log_softmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1605\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.96 GiB (GPU 0; 8.00 GiB total capacity; 5.19 GiB already allocated; 873.07 MiB free; 5.33 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bm_gen.init_hidden_layer(features_tensor.size(0))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(bm_gen.parameters(), lr=0.01)\n",
    "output = bm_gen(features_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our model\n",
    "song_file_name = \"(706a)_Redo_(TV_Size)_ReZero_Opening_-_Konomi_Suzuki\"\n",
    "# (7067)_Sorairo_Days_(TV_Size)_Gurren_Lagann_Opening_-_Shoko_Nakagawa\n",
    "song_info = maps_df.loc[maps_df['key'] == '706a']\n",
    "# print(song_info)\n",
    "bpm = (song_info['bpm'].values)[0]\n",
    "print(bpm)\n",
    "\n",
    "with ZipFile('../Data_Gather_Filter_Download/Zip_Songs_Data/{}.zip'.format(song_file_name)) as folder:\n",
    "    with folder.open('ExpertPlus.dat') as dat_file:\n",
    "        dat_json = json.load(dat_file)\n",
    "        placements = get_note_placements_by_index(dat_json, most_common_placements)\n",
    "        # print(placements)\n",
    "    folder.extract('song.egg')\n",
    "\n",
    "    # get_map_from_song('song.egg', start_time=0, bpm=190)\n",
    "    # os.remove('song.egg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load('song.egg')\n",
    "length = y.shape[0] / sr # Song length according to librosa in secs (doesn't match given length for some reason)\n",
    "y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
    "tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr, trim=False, bpm=bpm)\n",
    "# We are analyzing the song in time points of 16th beats\n",
    "total_num_beats = time_to_beat(length, bpm) # Length in terms of seconds\n",
    "length_one_beat = beat_to_time(1, bpm)\n",
    "time_points = np.arange(0, length, length_one_beat / 16) # 16th beats\n",
    "\n",
    "# print(total_num_beats)\n",
    "# print(length_one_beat)\n",
    "# print(length_one_beat / 16)\n",
    "# print(tempo, len(y), len(beat_frames), \"\\n\", beat_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce n_mels to avoid empty filters in mel frequency basis and to match the size of chrormagram\n",
    "melspectrogram = librosa.feature.melspectrogram(y=y_percussive, sr=sr, n_mels=12, fmax=65.4)\n",
    "chromagram = librosa.feature.chroma_cqt(y=y_harmonic, sr=sr)\n",
    "# mfcc = get_mfcc('song.egg', beat_to_time(next_item[0], bpm), beat_to_time(1, bpm), n_mfcc=1)\n",
    "\n",
    "# print(len(mfcc))\n",
    "# print(len(melspectrogram))\n",
    "# print(melspectrogram[4])\n",
    "# print(len(chromagram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 1.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Need to do it this way as some songs start with no noise so we can't naively just assuming that beat 0 is at time 0\n",
    "quarter_frames = np.array([])\n",
    "index = 0\n",
    "while index < len(beat_frames) - 1:\n",
    "    quarter_frames = np.append(quarter_frames, np.arange(beat_frames[index], beat_frames[index + 1], \n",
    "                                                         round((beat_frames[index + 1] - beat_frames[index]) / 4))[1:4])\n",
    "    index += 1\n",
    "    # if index < 2:\n",
    "    #     print(beat_frames[index])\n",
    "    #     print((beat_frames[index + 1] - beat_frames[index])/4)\n",
    "    #     print(np.arange(beat_frames[index], beat_frames[index+1], round((beat_frames[index+1]-beat_frames[index])/4))[1:4])\n",
    "\n",
    "beat_frames_with_quarter = np.concatenate((np.array([0]), beat_frames, quarter_frames), axis=0)\n",
    "beat_frames_with_quarter.sort() # We appended the quarter notes so we need to sort them into right spots\n",
    "beat_frames_with_quarter = beat_frames_with_quarter.astype(int)\n",
    "\n",
    "beat_frames_with_mel = librosa.util.sync(melspectrogram, beat_frames_with_quarter, aggregate=np.median)\n",
    "beat_frames_with_chroma = librosa.util.sync(chromagram, beat_frames_with_quarter, aggregate=np.median)\n",
    "\n",
    "# Want to extend it to 1/16 beats as 1/4 beats limits the max NPS by a lot\n",
    "# Using pandas to easily extend to 1/16 beats\n",
    "beat_num_with_16th_notes = np.arange(0, len(beat_frames) - 1, 1.0 / 16.0)\n",
    "beat_num = np.arange(0, len(beat_frames), 1)\n",
    "beats_num_df = pd.concat([pd.Series(beat_num, name='beat_num', dtype=int), pd.Series(beat_frames, name='beat_frame', dtype=int)], axis=1)\n",
    "mel_beats_df = pd.concat([pd.Series(beat_frames_with_quarter, name='quater_beat_frame'), pd.DataFrame(beat_frames_with_mel.T)], axis=1)\n",
    "chroma_beats_df = pd.concat([pd.Series(beat_frames_with_quarter, name='quater_beat_frame'), pd.DataFrame(beat_frames_with_chroma.T)], axis=1)\n",
    "# Dataframe with the beat number, frame number, mel data, and chroma data for the beat\n",
    "beats_df = beats_num_df.merge(mel_beats_df, how='outer', left_on='beat_frame', right_on='quater_beat_frame', sort=True)\n",
    "# Only need the quarter beat frames\n",
    "beats_df = beats_df.drop(columns=['beat_frame']) \n",
    "beats_df = beats_df.merge(chroma_beats_df, how='outer', on='quater_beat_frame', sort=True)\n",
    "# Removes the NaNs from the beat number column for next merge\n",
    "beats_df.interpolate(inplace=True) \n",
    "# Expand it to be 1/16 beats. Doing it in 1/4 beats saves a lot of computation with sync\n",
    "beats_num_16th_notes_df = pd.DataFrame(beat_num_with_16th_notes, columns=['beat_num'])\n",
    "beats_df = beats_df.merge(beats_num_16th_notes_df, how='outer', on='beat_num', sort=True)\n",
    "# Interpolate the quarter beat frame so we can more accuractly place the placements\n",
    "beats_df['quater_beat_frame'].interpolate(inplace=True) \n",
    "beats_df['quater_beat_frame'] = beats_df['quater_beat_frame'].round()\n",
    "beats_df = beats_df.fillna(method='pad') # Forward fill\n",
    "# Add placement column to store what note type is at that time\n",
    "beats_df.insert(1, 'placement', 0)\n",
    "# Add column for time since last note since this helps dicate what placements should be done\n",
    "beats_df.insert(3, 'time_since_last_note', 0.0)\n",
    "# print(beats_df.iloc[0:40, 0:3])\n",
    "# Now the computed audio values for each quarter note are spread among 1/16 notes so we can use sequences of 1/16 notes\n",
    "for timing, placement in placements.items():\n",
    "    time_in_frames = librosa.core.time_to_frames(beat_to_time(timing, bpm), sr=sr)\n",
    "    # print(timing, placement)\n",
    "    # print(time_in_frames)\n",
    "    try:\n",
    "        matching_frames = beats_df.loc[beats_df['quater_beat_frame'] == time_in_frames, 'placement']\n",
    "        if len(matching_frames) == 1:\n",
    "            beats_df.loc[beats_df['quater_beat_frame'] == time_in_frames, 'placement'] = placement\n",
    "        else:\n",
    "            placement_arr = [placement]\n",
    "            placement_arr.extend([0] * (len(matching_frames) - 1))\n",
    "            beats_df.loc[beats_df['quater_beat_frame'] == time_in_frames, 'placement'] = placement_arr\n",
    "        # print(beats_df.loc[beats_df['quater_beat_frame'] == time_in_frames, 'placement'])\n",
    "        # print(len(beats_df.loc[beats_df['quater_beat_frame'] == time_in_frames, 'placement']))\n",
    "        # print(type(beats_df.loc[beats_df['quater_beat_frame'] == time_in_frames, 'placement']))\n",
    "        # print('\\n')\n",
    "    except: # No exact note timing match\n",
    "        print(\"No exact match found\")\n",
    "        # Subtract the value and find the one closest to 0\n",
    "        closest_index = beats_df['quater_beat_frame'].sub(time_in_frames).abs().idxmin()\n",
    "        beats_df[closest_index]['placement'] = placement\n",
    "\n",
    "beats_df.loc[beats_df['placement'] == 16, 'placement'] = 0\n",
    "last_note_time = 0\n",
    "features = []\n",
    "for i in range(len(beats_df.index)):\n",
    "    # Set the time since the last note\n",
    "    time_in_s = librosa.core.frames_to_time(beats_df.at[i, 'quater_beat_frame'], sr=sr)\n",
    "    if beats_df.at[i, 'placement'] != 0 and beats_df.at[i, 'placement'] != 16:\n",
    "        last_note_time = time_in_s\n",
    "        # print(last_note_time)\n",
    "    if last_note_time != 0:\n",
    "        beats_df.at[i, 'time_since_last_note'] = (time_in_s - last_note_time)\n",
    "    # Extract the data from this row\n",
    "    features.append(beats_df.iloc[i, 3:].tolist())\n",
    "\n",
    "features = np.asarray(features)\n",
    "human_placements = beats_df['placement'].to_numpy()\n",
    "\n",
    "# output = np.concatenate((beat_frames_with_mel, beat_frames_with_chroma), axis=0)\n",
    "\n",
    "# print(len(features))\n",
    "# print(len(beats_df))\n",
    "# print(len(human_placements))\n",
    "# print(human_placements[:50])\n",
    "# print(features[0:2])\n",
    "# print(features)\n",
    "# print(beats_df.iloc[0:40, 0:3])\n",
    "# print(beats_df.dtypes)\n",
    "# print(librosa.time_to_frames(length))\n",
    "# print(len(beat_frames_with_quarter))    # Do match\n",
    "# print(len(output[9]))                   # Do match\n",
    "# print(len(beat_frames))      \n",
    "# print(len(beat_frames_with_chroma))\n",
    "# print(beat_frames_with_quarter[:18])\n",
    "# print(beat_frames[:10])\n",
    "# print(output[10][:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_points = librosa.core.time_to_frames(time_points, sr=sr)\n",
    "beat_frames_with_mel = librosa.util.sync(melspectrogram, frame_points, aggregate=np.median)\n",
    "beat_frames_with_chroma = librosa.util.sync(chromagram, frame_points, aggregate=np.median)\n",
    "output = np.concatenate((beat_frames_with_mel, beat_frames_with_chroma), axis=0)\n",
    "# print(len(beat_frames_with_mel))\n",
    "# print(len(beat_frames_with_chroma))\n",
    "print(len(frame_points))    # DONT MATCH????\n",
    "print(len(output[9]))       # DONT MATCH????\n",
    "print(output[10][:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sequences, placements_for_seq = split_into_sequences(features, human_placements, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4530\n25\n4019\n512\n25\n4019\n[[0.00000000e+00 0.00000000e+00 2.30849841e-23 ... 8.53028059e-01\n  8.19255352e-01 8.36956620e-01]\n [0.00000000e+00 0.00000000e+00 2.30849841e-23 ... 8.53028059e-01\n  8.19255352e-01 8.36956620e-01]\n [0.00000000e+00 0.00000000e+00 2.30849841e-23 ... 8.53028059e-01\n  8.19255352e-01 8.36956620e-01]\n ...\n [4.64399093e-02 0.00000000e+00 2.60810107e-01 ... 9.08461094e-01\n  8.72298896e-01 8.52924228e-01]\n [0.00000000e+00 0.00000000e+00 2.60810107e-01 ... 9.08461094e-01\n  8.72298896e-01 8.52924228e-01]\n [2.32199546e-02 0.00000000e+00 2.60810107e-01 ... 9.08461094e-01\n  8.72298896e-01 8.52924228e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(len(features))\n",
    "print(len(features[0]))\n",
    "print(len(feature_sequences))\n",
    "print(len(feature_sequences[0]))\n",
    "print(len(feature_sequences[0][0]))\n",
    "print(len(placements_for_seq))\n",
    "print(feature_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(4019, 512, 25)\n"
     ]
    }
   ],
   "source": [
    "print(feature_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Saved_Features/test/features.txt'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-454-c1b2e6d016a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Saved_Features/test/features.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Test\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Saved_Features/test/features.txt'"
     ]
    }
   ],
   "source": [
    "with open('Saved_Features/test/features.txt', 'w') as f:\n",
    "    f.write(\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test.npy', feature_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "OrderedDict([('features', array([[[ 0.00000000e+00,  0.00000000e+00,  2.30849841e-23, ...,\n          8.53028059e-01,  8.19255352e-01,  8.36956620e-01],\n        [ 0.00000000e+00,  0.00000000e+00,  2.30849841e-23, ...,\n          8.53028059e-01,  8.19255352e-01,  8.36956620e-01],\n        [ 0.00000000e+00,  0.00000000e+00,  2.30849841e-23, ...,\n          8.53028059e-01,  8.19255352e-01,  8.36956620e-01],\n        ...,\n        [ 4.64399093e-02,  0.00000000e+00,  2.60810107e-01, ...,\n          9.08461094e-01,  8.72298896e-01,  8.52924228e-01],\n        [ 0.00000000e+00,  0.00000000e+00,  2.60810107e-01, ...,\n          9.08461094e-01,  8.72298896e-01,  8.52924228e-01],\n        [ 2.32199546e-02,  0.00000000e+00,  2.60810107e-01, ...,\n          9.08461094e-01,  8.72298896e-01,  8.52924228e-01]],\n\n       [[ 0.00000000e+00,  0.00000000e+00,  2.30849841e-23, ...,\n          8.53028059e-01,  8.19255352e-01,  8.36956620e-01],\n        [ 0.00000000e+00,  0.00000000e+00,  2.30849841e-23, ...,\n          8.53028059e-01,  8.19255352e-01,  8.36956620e-01],\n        [ 0.00000000e+00,  0.00000000e+00,  2.30849841e-23, ...,\n          8.53028059e-01,  8.19255352e-01,  8.36956620e-01],\n        ...,\n        [ 0.00000000e+00,  0.00000000e+00,  2.60810107e-01, ...,\n          9.08461094e-01,  8.72298896e-01,  8.52924228e-01],\n        [ 2.32199546e-02,  0.00000000e+00,  2.60810107e-01, ...,\n          9.08461094e-01,  8.72298896e-01,  8.52924228e-01],\n        [ 4.64399093e-02,  0.00000000e+00,  2.45310378e+00, ...,\n          9.50684845e-01,  9.34511185e-01,  9.17855620e-01]],\n\n       [[ 0.00000000e+00,  0.00000000e+00,  2.30849841e-23, ...,\n          8.53028059e-01,  8.19255352e-01,  8.36956620e-01],\n        [ 0.00000000e+00,  0.00000000e+00,  2.30849841e-23, ...,\n          8.53028059e-01,  8.19255352e-01,  8.36956620e-01],\n        [ 0.00000000e+00,  0.00000000e+00,  3.87054314e-14, ...,\n          7.01185167e-01,  6.80216849e-01,  7.84609914e-01],\n        ...,\n        [ 2.32199546e-02,  0.00000000e+00,  2.60810107e-01, ...,\n          9.08461094e-01,  8.72298896e-01,  8.52924228e-01],\n        [ 4.64399093e-02,  0.00000000e+00,  2.45310378e+00, ...,\n          9.50684845e-01,  9.34511185e-01,  9.17855620e-01],\n        [ 6.96598639e-02,  0.00000000e+00,  2.45310378e+00, ...,\n          9.50684845e-01,  9.34511185e-01,  9.17855620e-01]],\n\n       ...,\n\n       [[ 4.64399093e-02,  0.00000000e+00,  5.18960333e+00, ...,\n          7.74391651e-01,  6.63668513e-01,  2.71326661e-01],\n        [ 0.00000000e+00,  0.00000000e+00,  5.18960333e+00, ...,\n          7.74391651e-01,  6.63668513e-01,  2.71326661e-01],\n        [ 2.32199546e-02,  0.00000000e+00,  5.18960333e+00, ...,\n          7.74391651e-01,  6.63668513e-01,  2.71326661e-01],\n        ...,\n        [ 8.35918367e-01,  0.00000000e+00,  6.82085602e-06, ...,\n          2.05423087e-01,  5.29983282e-01,  1.00000000e+00],\n        [ 8.35918367e-01,  0.00000000e+00,  6.82085602e-06, ...,\n          2.05423087e-01,  5.29983282e-01,  1.00000000e+00],\n        [ 8.35918367e-01,  0.00000000e+00,  6.82085602e-06, ...,\n          2.05423087e-01,  5.29983282e-01,  1.00000000e+00]],\n\n       [[ 0.00000000e+00,  0.00000000e+00,  5.18960333e+00, ...,\n          7.74391651e-01,  6.63668513e-01,  2.71326661e-01],\n        [ 2.32199546e-02,  0.00000000e+00,  5.18960333e+00, ...,\n          7.74391651e-01,  6.63668513e-01,  2.71326661e-01],\n        [ 4.64399093e-02,  0.00000000e+00,  5.18960333e+00, ...,\n          7.74391651e-01,  6.63668513e-01,  2.71326661e-01],\n        ...,\n        [ 8.35918367e-01,  0.00000000e+00,  6.82085602e-06, ...,\n          2.05423087e-01,  5.29983282e-01,  1.00000000e+00],\n        [ 8.35918367e-01,  0.00000000e+00,  6.82085602e-06, ...,\n          2.05423087e-01,  5.29983282e-01,  1.00000000e+00],\n        [ 8.59138322e-01,  0.00000000e+00,  2.37200675e-06, ...,\n          4.77202713e-01,  6.48056209e-01,  8.27999830e-01]],\n\n       [[ 2.32199546e-02,  0.00000000e+00,  5.18960333e+00, ...,\n          7.74391651e-01,  6.63668513e-01,  2.71326661e-01],\n        [ 4.64399093e-02,  0.00000000e+00,  5.18960333e+00, ...,\n          7.74391651e-01,  6.63668513e-01,  2.71326661e-01],\n        [ 0.00000000e+00,  0.00000000e+00,  5.88221431e-01, ...,\n          3.61085236e-01,  2.56216586e-01,  1.83728784e-01],\n        ...,\n        [ 8.35918367e-01,  0.00000000e+00,  6.82085602e-06, ...,\n          2.05423087e-01,  5.29983282e-01,  1.00000000e+00],\n        [ 8.59138322e-01,  0.00000000e+00,  2.37200675e-06, ...,\n          4.77202713e-01,  6.48056209e-01,  8.27999830e-01],\n        [-9.01863039e+01,  0.00000000e+00,  4.28342131e-22, ...,\n          5.79528451e-01,  6.74606383e-01,  9.40546870e-01]]])), ('target', array([0, 0, 0, ..., 0, 7, 0], dtype=int64))])\n<class 'builtin_function_or_method'>\n<built-in method keys of collections.OrderedDict object at 0x0000022FDEE17BC0>\n"
     ]
    }
   ],
   "source": [
    "test_dict = OrderedDict({'features' : feature_sequences, 'target' : placements_for_seq})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = [[0] * 25 ]\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}