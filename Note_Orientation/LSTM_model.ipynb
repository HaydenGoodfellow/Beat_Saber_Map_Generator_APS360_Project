{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "58c7c12f8aebb3b799780e2a952497b720e238e6f3ceeaa6cb2f46c86be22697"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import time\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"\\\\Map_Processing\")\n",
    "    sys.path.append(module_path+\"\\\\Note_Timing\")\n",
    "\n",
    "# import importlib\n",
    "# importlib.reload(sys.modules['analyze_notes'])\n",
    "# importlib.reload(sys.modules['onset_detection'])\n",
    "\n",
    "from analyze_notes import get_note_placements_by_index \n",
    "from onset_detection import get_onset_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get maps dataframe from the pickle file\n",
    "maps_df = pd.read_pickle(\"../Data_Gather_Filter_Download/downloaded_maps_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Note_Orientation/most_common_placements.pkl', 'rb') as f:\n",
    "    most_common_placements = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of training maps: 11250. Numer of validation maps: 3000. Number of testing maps: 750\n"
     ]
    }
   ],
   "source": [
    "#==================================== Dataset Settings ====================================#\n",
    "# Note: We have to use a subset of all our songs as 15k maps would take days to train\n",
    "total_data_size = 15000 # Number of maps to use in across all datasets\n",
    "val_split = 0.2         # Percentage of data put into validation set\n",
    "test_split = 0.05       # Percentage of data put into testing set\n",
    "#==========================================================================================#\n",
    "\n",
    "# Split our data into training and test/val which we will split again\n",
    "train_df, val_test_df = train_test_split(maps_df[:total_data_size], test_size=val_split + test_split)\n",
    "\n",
    "# Split the validation and testing data apart into their own respective sets\n",
    "val_df, test_df = train_test_split(val_test_df, test_size=(test_split/(val_split + test_split)))\n",
    "\n",
    "print(\"Number of training maps: {}. Numer of validation maps: {}. Number of testing maps: {}\".format(len(train_df), len(val_df), len(test_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_dataset = Dict_Dataset(feature_sequences, placements_for_seq)\n",
    "# Create the data loader we will use\n",
    "train_loader = torch.utils.data.DataLoader(feature_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "\n",
    "for data in train_loader:\n",
    "    print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class beatmap_generator(nn.Module):\n",
    "    def __init__(self, input_size, output_size, seq_size, hidden_size, num_layers=1, dropout=0):\n",
    "        super(beatmap_generator, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.seq_size = seq_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # LSTM model\n",
    "        self.net = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "        # Converts output back to valid index into most common notes\n",
    "        self.decoder = nn.Linear(hidden_size * seq_size, output_size)\n",
    "    \n",
    "    def init_hidden_layer(self, batch_size):\n",
    "        self.batch_size = batch_size # Have to set it here\n",
    "        hidden_init = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)\n",
    "        cell_init = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)\n",
    "        self.hidden = (hidden_init, cell_init)\n",
    "\n",
    "    def forward(self, data):\n",
    "        output, self.hidden = self.net(data, self.hidden)   # get the next output and hidden state\n",
    "        output = output.contiguous().view(data.size(0), -1)\n",
    "        output = self.decoder(output)                       # predict distribution over next tokens\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CUDA available, using GPU\n"
     ]
    }
   ],
   "source": [
    "#===================================== Model Settings =====================================#\n",
    "input_size = 25                             # Number of features in input\n",
    "output_size = len(most_common_placements)   # Number of possible outputs for model\n",
    "seq_size = 512                              # Length of note sequences passed into model\n",
    "hidden_size = 128                           # Number of nodes in hidden layer\n",
    "num_layers = 2                              # Number of LSTM layers for stacked LSTM\n",
    "dropout = 0                                 # Probability of droping weights in the dropout layer\n",
    "#==========================================================================================#\n",
    "\n",
    "bm_gen = beatmap_generator(input_size, output_size, seq_size, hidden_size, num_layers, dropout)\n",
    "# Set it to use the GPU\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"CUDA available, using GPU\")\n",
    "    bm_gen = bm_gen.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from tutorial 5 and 6 code and tutorial 3 and 4\n",
    "def train_network(model, train_df, val_df, num_epochs=5, learning_rate=learning_rate, batch_size=16):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    losses, train_acc, valid_acc = np.zeros(num_epochs), np.zeros(num_epochs), np.zeros(num_epochs)\n",
    "    num_songs = 0\n",
    "    epochs = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for beatmap in train_df.itertuples():\n",
    "            start_time = time.time()\n",
    "            diff = beatmap.difficulty\n",
    "            diff = diff[:1].upper() + diff[1:] # Capitialize the first character only\n",
    "            if 'extensions' in beatmap.requirements:\n",
    "                continue # Catches a few maps that shouldn't have made it here\n",
    "            # See if the features have already been calculated\n",
    "            try:\n",
    "                folder_name = beatmap.file_path.rsplit('/', 1)[-1]\n",
    "                folder_name = folder_name.split('.')[0]\n",
    "                folder_p = Path('Saved_Features/v{}_{}/'.format(features_version, folder_name))\n",
    "                file_p = folder_p / '{}.npy'.format(diff)\n",
    "                print(file_p)\n",
    "                features_dict = np.load(file_p, allow_pickle=True)\n",
    "                print(\"Successfully opened file for {} on {} difficulty\".format(beatmap.song_name, diff))\n",
    "                features = features_dict.item().get('features')\n",
    "                padded_placements = features_dict.item().get('placements')\n",
    "            \n",
    "            except Exception as e: # If they haven't been then calculate them now\n",
    "                print(\"Calculating features and placements for {} on {} difficulty. Exception: {}\".format(beatmap.song_name, diff, e))\n",
    "                with ZipFile('../Data_Gather_Filter_Download/{}'.format(beatmap.file_path)) as folder:\n",
    "                    filenames = folder.namelist()\n",
    "                    with folder.open('{}.dat'.format(diff)) as dat_file:\n",
    "                        dat_json = json.load(dat_file)\n",
    "                        placements = get_note_placements_by_index(dat_json, most_common_placements)\n",
    "                    song_path = list(filter(lambda x: re.match(r'(^.+\\.(egg|ogg|mp4|mp3))', x, flags=re.I), filenames))[0]\n",
    "                    folder.extract(song_path)\n",
    "                    features, padded_placements = get_features(song_path, bpm=beatmap.bpm, placements=placements, \n",
    "                                                               save_data=True, difficulty=diff, folder_path=beatmap.file_path)\n",
    "                    os.remove(song_path)\n",
    "            # Create sequence arrays for different permutations\n",
    "            feature_sequences, placements_for_seq = split_into_sequences(features, padded_placements, 512)\n",
    "            feature_tensor = torch.from_numpy(feature_sequences).float()\n",
    "            placements_for_seq_tensor = torch.from_numpy(placements_for_seq).long()\n",
    "            # Create the data loader we will use\n",
    "            train_loader = torch.utils.data.DataLoader(feature_tensor, batch_size=batch_size, num_workers=num_workers)\n",
    "            label_loader = torch.utils.data.DataLoader(placements_for_seq_tensor, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "            \n",
    "            print(\"Looping over {} batches\".format(len(train_loader)))\n",
    "            for feature_batch, target_batch in zip(train_loader, label_loader):\n",
    "                if torch.cuda.is_available():\n",
    "                    feature_batch = feature_batch.cuda()\n",
    "                    target_batch = target_batch.cuda()\n",
    "                model.init_hidden_layer(feature_batch.size(0))\n",
    "                output = model(feature_batch)\n",
    "                # print('output shape: {}'.format(output.shape))\n",
    "                # output = output.view(output.size(0) * output.size(1), output.size(2))\n",
    "                # print('output shape: {}'.format(output.shape))\n",
    "                # print('target shape: {}'.format(target_batch.shape))\n",
    "                # print('Max in target {}, min in target {}'.format(torch.argmax(target_batch), torch.argmin(target_batch)))\n",
    "                loss = criterion(output, target_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            end_time = time.time()\n",
    "            print(\"Time taken for song: {:.2f} seconds\".format(end_time - start_time))\n",
    "        losses[epoch] = float(loss)    \n",
    "        epochs.append(epoch)\n",
    "        train_acc[epoch] = get_accuracy(model, train_iter)\n",
    "        valid_acc[epoch] = get_accuracy(model, valid_iter)\n",
    "        print(\"Epoch %d; Loss %f; Train Acc %f; Val Acc %f\" % (\n",
    "            epoch+1, loss, train_acc[epoch], valid_acc[epoch]))\n",
    "\n",
    "    # plotting\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(losses, label=\"Train\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(epochs, train_acc, label=\"Train\")\n",
    "    plt.plot(epochs, valid_acc, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-2cbad43a2665>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtrain_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbm_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-92-09060dcb9cdf>\u001b[0m in \u001b[0;36mtrain_network\u001b[1;34m(model, train_df, val_df, num_epochs, learning_rate, batch_size)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbeatmap\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitertuples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbeatmap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdifficulty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# Capitialize the first character only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "#==================================== Training Settings ===================================#\n",
    "learning_rate = 0.004   # Learning rate\n",
    "num_epochs = 5          # Number of epochs\n",
    "batch_size = 32         # Number of sequences to batch together\n",
    "num_workers = 1         # Number of workers to load the data\n",
    "features_version = 1    # If I make any large changes that require re-calculating features\n",
    "#==========================================================================================#\n",
    "\n",
    "\n",
    "train_network(bm_gen, train_df, val_df, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(song_path, bpm, placements=None, save_data=False, difficulty='expertPlus', version=features_version, folder_path=''):\n",
    "    beat_frames, sr, melspectrogram, chromagram = get_audio_data(song_path, bpm)\n",
    "    quarter_frames = np.array([]) # Array to store data for every quarter note\n",
    "    index = 0\n",
    "    while index < len(beat_frames) - 1:\n",
    "        quarter_frames = np.append(quarter_frames, np.arange(beat_frames[index], beat_frames[index + 1], \n",
    "                                                            round((beat_frames[index + 1] - beat_frames[index]) / 4))[1:4])\n",
    "        index += 1\n",
    "    \n",
    "    beat_frames_with_quarter = np.concatenate((np.array([0]), beat_frames, quarter_frames), axis=0)\n",
    "    beat_frames_with_quarter.sort() # We appended the quarter notes so we need to sort them into right spots\n",
    "    beat_frames_with_quarter = beat_frames_with_quarter.astype(int)\n",
    "\n",
    "    beat_frames_with_mel = librosa.util.sync(melspectrogram, beat_frames_with_quarter, aggregate=np.median)\n",
    "    beat_frames_with_chroma = librosa.util.sync(chromagram, beat_frames_with_quarter, aggregate=np.median)\n",
    "\n",
    "    # Want to extend it to 1/16 beats as 1/4 beats limits the max NPS by a lot\n",
    "    # Using pandas to easily extend to 1/16 beats\n",
    "    beat_num_with_16th_notes = np.arange(0, len(beat_frames) - 1, 1.0 / 16.0)\n",
    "    beat_num = np.arange(0, len(beat_frames), 1)\n",
    "    beats_num_df = pd.concat([pd.Series(beat_num, name='beat_num', dtype=int), pd.Series(beat_frames, name='beat_frame', dtype=int)], axis=1)\n",
    "    mel_beats_df = pd.concat([pd.Series(beat_frames_with_quarter, name='quater_beat_frame'), pd.DataFrame(beat_frames_with_mel.T)], axis=1)\n",
    "    chroma_beats_df = pd.concat([pd.Series(beat_frames_with_quarter, name='quater_beat_frame'), pd.DataFrame(beat_frames_with_chroma.T)], axis=1)\n",
    "\n",
    "    # Dataframe with the beat number, frame number, mel data, and chroma data for the beat\n",
    "    beats_df = beats_num_df.merge(mel_beats_df, how='outer', left_on='beat_frame', right_on='quater_beat_frame', sort=True)\n",
    "\n",
    "    # Only need the quarter beat frames\n",
    "    beats_df = beats_df.drop(columns=['beat_frame']) \n",
    "    beats_df = beats_df.merge(chroma_beats_df, how='outer', on='quater_beat_frame', sort=True)\n",
    "\n",
    "    # Removes the NaNs from the beat number column for next merge\n",
    "    beats_df.interpolate(inplace=True) \n",
    "\n",
    "    # Expand it to be 1/16 beats. Doing it in 1/4 beats saves a lot of computation with sync\n",
    "    beats_num_16th_notes_df = pd.DataFrame(beat_num_with_16th_notes, columns=['beat_num'])\n",
    "    beats_df = beats_df.merge(beats_num_16th_notes_df, how='outer', on='beat_num', sort=True)\n",
    "\n",
    "    # Interpolate the quarter beat frame so we can more accuractly place the placements\n",
    "    beats_df['quater_beat_frame'].interpolate(inplace=True) \n",
    "    beats_df['quater_beat_frame'] = beats_df['quater_beat_frame'].round()\n",
    "    beats_df = beats_df.fillna(method='pad') # Forward fill\n",
    "\n",
    "    # Add placement column to store what note type is at that time\n",
    "    beats_df.insert(1, 'placement', 0)\n",
    "    # Add column for time since last note since this helps dicate what placements should be done\n",
    "    beats_df.insert(3, 'time_since_last_note', 0.0)\n",
    "\n",
    "    # Now the computed audio values for each quarter note are spread among 1/16 notes so we can use sequences of 1/16 notes\n",
    "    for timing, placement in placements.items():\n",
    "        time_in_frames = librosa.core.time_to_frames(beat_to_time(timing, bpm), sr=sr)\n",
    "        try:\n",
    "            matching_frames = beats_df.loc[beats_df['quater_beat_frame'] == time_in_frames, 'placement']\n",
    "            if len(matching_frames) == 1:\n",
    "                beats_df.loc[beats_df['quater_beat_frame'] == time_in_frames, 'placement'] = placement\n",
    "            else: # Must be more than one quater beat on that frame\n",
    "                placement_arr = [placement]\n",
    "                placement_arr.extend([0] * (len(matching_frames) - 1))\n",
    "                beats_df.loc[beats_df['quater_beat_frame'] == time_in_frames, 'placement'] = placement_arr\n",
    "        except: # No exact note timing match\n",
    "            # Subtract the value and find the one closest to 0\n",
    "            closest_index = beats_df['quater_beat_frame'].sub(time_in_frames).abs().idxmin()\n",
    "            beats_df[closest_index]['placement'] = placement\n",
    "    # Set the placements with 16 to 0 as they represent the same board but are easier to distinguish\n",
    "    beats_df.loc[beats_df['placement'] == 16, 'placement'] = 0\n",
    "\n",
    "    # Extract from pandas into list which we can turn into tensor later\n",
    "    features = []\n",
    "    last_note_time = 0\n",
    "    for i in range(len(beats_df.index)):\n",
    "        # Set the time since the last note\n",
    "        time_in_s = librosa.core.frames_to_time(beats_df.at[i, 'quater_beat_frame'], sr=sr)\n",
    "        if beats_df.at[i, 'placement'] != 0 and beats_df.at[i, 'placement'] != 16:\n",
    "            last_note_time = time_in_s\n",
    "        if last_note_time != 0:\n",
    "            beats_df.at[i, 'time_since_last_note'] = (time_in_s - last_note_time)\n",
    "        # Extract the data from this row\n",
    "        features.append(beats_df.iloc[i, 3:].tolist())\n",
    "\n",
    "    # Convert to numpy arrays for easier slicing later\n",
    "    features = np.asarray(features)\n",
    "    # Placements made by the human mapper\n",
    "    human_placements = beats_df['placement'].to_numpy()\n",
    "\n",
    "    if save_data:\n",
    "        feat_dict = {'features' : features, 'placements' : human_placements}\n",
    "        folder_name = folder_path.rsplit('/', 1)[-1]\n",
    "        folder_name = folder_name.split('.')[0]\n",
    "        path = 'Saved_Features/v{}_{}/'.format(version, folder_name)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        np.save(os.path.join(path, '{}.npy'.format(difficulty)), feat_dict)\n",
    "\n",
    "    return features, human_placements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_data(song_path, bpm):\n",
    "    y, sr = librosa.load(song_path)\n",
    "    length = y.shape[0] / sr # Song length according to librosa in secs (doesn't match given length for some reason)\n",
    "    y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
    "    tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr, trim=False, bpm=bpm)\n",
    "    # Reduce n_mels to avoid empty filters in mel frequency basis and to match the size of chrormagram\n",
    "    melspectrogram = librosa.feature.melspectrogram(y=y_percussive, sr=sr, n_mels=12, fmax=65.4)\n",
    "    chromagram = librosa.feature.chroma_cqt(y=y_harmonic, sr=sr)\n",
    "    return beat_frames, sr, melspectrogram, chromagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sequences(features, placements, seq_len):\n",
    "    feature_sequences = []\n",
    "    placements_for_seq = []\n",
    "    for i in range(len(features)):\n",
    "        end_index = i + seq_len\n",
    "        if end_index > len(features):\n",
    "            break\n",
    "        feature_seq = features[i:end_index, :] \n",
    "        placement_for_seq = placements[i]\n",
    "        feature_sequences.append(feature_seq)\n",
    "        placements_for_seq.append(placement_for_seq)\n",
    "    return np.array(feature_sequences), np.array(placements_for_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(net, data):\n",
    "    correct, total = 0, 0\n",
    "    for sms, labels in data:\n",
    "        output = net(sms[0])\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        total += labels.shape[0]\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to do some conversions\n",
    "# Time in seconds to beat number\n",
    "def time_to_beat(note_time, bpm):\n",
    "    return (note_time / 60) * bpm\n",
    "\n",
    "# Beat number to seconds\n",
    "def beat_to_time(beat_time, bpm):\n",
    "    return (beat_time / bpm) * 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Dict_Dataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, features, targets):\n",
    "#         self.targets = targets\n",
    "#         self.features = features\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         print('index : {}'.format(index))\n",
    "#         if index >= self.__len__():\n",
    "#             raise IndexError\n",
    "#         feature = self.features[index]\n",
    "#         target = self.targets[index]\n",
    "#         print('Index: {}. Got feature and target {}'.format(index, self.targets[index]))\n",
    "#         return {'feature' : feature, 'target' : target}\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([512, 512, 25])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_batch = feature_sequences[0:512,:,:]\n",
    "features_tensor = torch.tensor(feature_batch, dtype=torch.float32)\n",
    "if torch.cuda.is_available():\n",
    "    features_tensor = features_tensor.cuda()\n",
    "print(features_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.96 GiB (GPU 0; 8.00 GiB total capacity; 5.19 GiB already allocated; 873.07 MiB free; 5.33 GiB reserved in total by PyTorch)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-438-e472262bb6dd>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# get the next output and hidden state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;31m# predict distribution over next tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'log_softmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1605\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.96 GiB (GPU 0; 8.00 GiB total capacity; 5.19 GiB already allocated; 873.07 MiB free; 5.33 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bm_gen.init_hidden_layer(features_tensor.size(0))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(bm_gen.parameters(), lr=0.01)\n",
    "output = bm_gen(features_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns file path to folder containing all files needed to play song made by model\n",
    "def get_map_from_song(song_file, output_file_path='Generated_Maps/Expert.dat', start_time=2, bpm=0):\n",
    "    # Get the onset times where we will place notes\n",
    "    onset_times = get_onset_times(song_file, min_sep=0.1)\n",
    "    num_before = len(onset_times)\n",
    "    onset_times = np.delete(onset_times, np.where(onset_times <= start_time))\n",
    "    print(\"Removed {} onset times for being before the specified start time\".format(num_before - len(onset_times)))\n",
    "    # If the bpm is not provided then we calculate it ourselves\n",
    "    if bpm == 0:\n",
    "        y, samp_rate = librosa.load(song_file)\n",
    "        bpm = librosa.beat.tempo(y=y, sr=samp_rate)\n",
    "        print(\"Got a bpm of {}\".format(bpm))\n",
    "    # Determine the notes we should place\n",
    "    \n",
    "\n",
    "    # Create dictionary with time key and notes values\n",
    "    notes_at_times = OrderedDict(zip(onset_times, notes_list))\n",
    "    notes_as_json = convert_notes_string_to_valid_json(notes_at_times, bpm)\n",
    "    with open(output_file_path, 'w') as dat_file:\n",
    "        dat_data = {\"_version\": \"2.2.0\",\n",
    "                    \"_customData\": {\n",
    "                        \"_time\": '',\n",
    "                        \"_BPMChanges\": [],\n",
    "                        \"_bookmarks\": []\n",
    "                        },\n",
    "                    \"_events\": [],\n",
    "                    \"_notes\": notes_as_json,\n",
    "                    \"_obstacles\": [],\n",
    "                    \"_waypoints\": []\n",
    "                    }\n",
    "        json.dump(dat_data, dat_file)\n",
    "    \n",
    "    print(\"Number of notes placed: {}\\nNumber of unique note placements: {}\\nApprox. notes per second: {}\".format(\n",
    "            len(notes_as_json),\n",
    "            len(set(notes_list)),\n",
    "            len(notes_as_json) / np.amax(onset_times)\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our model\n",
    "song_file_name = \"(706a)_Redo_(TV_Size)_ReZero_Opening_-_Konomi_Suzuki\"\n",
    "# (7067)_Sorairo_Days_(TV_Size)_Gurren_Lagann_Opening_-_Shoko_Nakagawa\n",
    "song_info = maps_df.loc[maps_df['key'] == '706a']\n",
    "# print(song_info)\n",
    "bpm = (song_info['bpm'].values)[0]\n",
    "print(bpm)\n",
    "\n",
    "with ZipFile('../Data_Gather_Filter_Download/Zip_Songs_Data/{}.zip'.format(song_file_name)) as folder:\n",
    "    with folder.open('ExpertPlus.dat') as dat_file:\n",
    "        dat_json = json.load(dat_file)\n",
    "        placements = get_note_placements_by_index(dat_json, most_common_placements)\n",
    "        # print(placements)\n",
    "    folder.extract('song.egg')\n",
    "\n",
    "    # get_map_from_song('song.egg', start_time=0, bpm=190)\n",
    "    # os.remove('song.egg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load('song.egg')\n",
    "length = y.shape[0] / sr # Song length according to librosa in secs (doesn't match given length for some reason)\n",
    "y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
    "tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr, trim=False, bpm=bpm)\n",
    "# We are analyzing the song in time points of 16th beats\n",
    "total_num_beats = time_to_beat(length, bpm) # Length in terms of seconds\n",
    "length_one_beat = beat_to_time(1, bpm)\n",
    "time_points = np.arange(0, length, length_one_beat / 16) # 16th beats\n",
    "\n",
    "# print(total_num_beats)\n",
    "# print(length_one_beat)\n",
    "# print(length_one_beat / 16)\n",
    "# print(tempo, len(y), len(beat_frames), \"\\n\", beat_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce n_mels to avoid empty filters in mel frequency basis and to match the size of chrormagram\n",
    "melspectrogram = librosa.feature.melspectrogram(y=y_percussive, sr=sr, n_mels=12, fmax=65.4)\n",
    "chromagram = librosa.feature.chroma_cqt(y=y_harmonic, sr=sr)\n",
    "# mfcc = get_mfcc('song.egg', beat_to_time(next_item[0], bpm), beat_to_time(1, bpm), n_mfcc=1)\n",
    "\n",
    "# print(len(mfcc))\n",
    "# print(len(melspectrogram))\n",
    "# print(melspectrogram[4])\n",
    "# print(len(chromagram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 1.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Need to do it this way as some songs start with no noise so we can't naively just assuming that beat 0 is at time 0\n",
    "quarter_frames = np.array([])\n",
    "index = 0\n",
    "while index < len(beat_frames) - 1:\n",
    "    quarter_frames = np.append(quarter_frames, np.arange(beat_frames[index], beat_frames[index + 1], \n",
    "                                                         round((beat_frames[index + 1] - beat_frames[index]) / 4))[1:4])\n",
    "    index += 1\n",
    "    # if index < 2:\n",
    "    #     print(beat_frames[index])\n",
    "    #     print((beat_frames[index + 1] - beat_frames[index])/4)\n",
    "    #     print(np.arange(beat_frames[index], beat_frames[index+1], round((beat_frames[index+1]-beat_frames[index])/4))[1:4])\n",
    "\n",
    "beat_frames_with_quarter = np.concatenate((np.array([0]), beat_frames, quarter_frames), axis=0)\n",
    "beat_frames_with_quarter.sort() # We appended the quarter notes so we need to sort them into right spots\n",
    "beat_frames_with_quarter = beat_frames_with_quarter.astype(int)\n",
    "\n",
    "beat_frames_with_mel = librosa.util.sync(melspectrogram, beat_frames_with_quarter, aggregate=np.median)\n",
    "beat_frames_with_chroma = librosa.util.sync(chromagram, beat_frames_with_quarter, aggregate=np.median)\n",
    "\n",
    "# Want to extend it to 1/16 beats as 1/4 beats limits the max NPS by a lot\n",
    "# Using pandas to easily extend to 1/16 beats\n",
    "beat_num_with_16th_notes = np.arange(0, len(beat_frames) - 1, 1.0 / 16.0)\n",
    "beat_num = np.arange(0, len(beat_frames), 1)\n",
    "beats_num_df = pd.concat([pd.Series(beat_num, name='beat_num', dtype=int), pd.Series(beat_frames, name='beat_frame', dtype=int)], axis=1)\n",
    "mel_beats_df = pd.concat([pd.Series(beat_frames_with_quarter, name='quater_beat_frame'), pd.DataFrame(beat_frames_with_mel.T)], axis=1)\n",
    "chroma_beats_df = pd.concat([pd.Series(beat_frames_with_quarter, name='quater_beat_frame'), pd.DataFrame(beat_frames_with_chroma.T)], axis=1)\n",
    "# Dataframe with the beat number, frame number, mel data, and chroma data for the beat\n",
    "beats_df = beats_num_df.merge(mel_beats_df, how='outer', left_on='beat_frame', right_on='quater_beat_frame', sort=True)\n",
    "# Only need the quarter beat frames\n",
    "beats_df = beats_df.drop(columns=['beat_frame']) \n",
    "beats_df = beats_df.merge(chroma_beats_df, how='outer', on='quater_beat_frame', sort=True)\n",
    "# Removes the NaNs from the beat number column for next merge\n",
    "beats_df.interpolate(inplace=True) \n",
    "# Expand it to be 1/16 beats. Doing it in 1/4 beats saves a lot of computation with sync\n",
    "beats_num_16th_notes_df = pd.DataFrame(beat_num_with_16th_notes, columns=['beat_num'])\n",
    "beats_df = beats_df.merge(beats_num_16th_notes_df, how='outer', on='beat_num', sort=True)\n",
    "# Interpolate the quarter beat frame so we can more accuractly place the placements\n",
    "beats_df['quater_beat_frame'].interpolate(inplace=True) \n",
    "beats_df['quater_beat_frame'] = beats_df['quater_beat_frame'].round()\n",
    "beats_df = beats_df.fillna(method='pad') # Forward fill\n",
    "# Add placement column to store what note type is at that time\n",
    "beats_df.insert(1, 'placement', 0)\n",
    "# Add column for time since last note since this helps dicate what placements should be done\n",
    "beats_df.insert(3, 'time_since_last_note', 0.0)\n",
    "# print(beats_df.iloc[0:40, 0:3])\n",
    "# Now the computed audio values for each quarter note are spread among 1/16 notes so we can use sequences of 1/16 notes\n",
    "for timing, placement in placements.items():\n",
    "    time_in_frames = librosa.core.time_to_frames(beat_to_time(timing, bpm), sr=sr)\n",
    "    # print(timing, placement)\n",
    "    # print(time_in_frames)\n",
    "    try:\n",
    "        matching_frames = beats_df.loc[beats_df['quater_beat_frame'] == time_in_frames, 'placement']\n",
    "        if len(matching_frames) == 1:\n",
    "            beats_df.loc[beats_df['quater_beat_frame'] == time_in_frames, 'placement'] = placement\n",
    "        else:\n",
    "            placement_arr = [placement]\n",
    "            placement_arr.extend([0] * (len(matching_frames) - 1))\n",
    "            beats_df.loc[beats_df['quater_beat_frame'] == time_in_frames, 'placement'] = placement_arr\n",
    "        # print(beats_df.loc[beats_df['quater_beat_frame'] == time_in_frames, 'placement'])\n",
    "        # print(len(beats_df.loc[beats_df['quater_beat_frame'] == time_in_frames, 'placement']))\n",
    "        # print(type(beats_df.loc[beats_df['quater_beat_frame'] == time_in_frames, 'placement']))\n",
    "        # print('\\n')\n",
    "    except: # No exact note timing match\n",
    "        print(\"No exact match found\")\n",
    "        # Subtract the value and find the one closest to 0\n",
    "        closest_index = beats_df['quater_beat_frame'].sub(time_in_frames).abs().idxmin()\n",
    "        beats_df[closest_index]['placement'] = placement\n",
    "\n",
    "beats_df.loc[beats_df['placement'] == 16, 'placement'] = 0\n",
    "last_note_time = 0\n",
    "features = []\n",
    "for i in range(len(beats_df.index)):\n",
    "    # Set the time since the last note\n",
    "    time_in_s = librosa.core.frames_to_time(beats_df.at[i, 'quater_beat_frame'], sr=sr)\n",
    "    if beats_df.at[i, 'placement'] != 0 and beats_df.at[i, 'placement'] != 16:\n",
    "        last_note_time = time_in_s\n",
    "        # print(last_note_time)\n",
    "    if last_note_time != 0:\n",
    "        beats_df.at[i, 'time_since_last_note'] = (time_in_s - last_note_time)\n",
    "    # Extract the data from this row\n",
    "    features.append(beats_df.iloc[i, 3:].tolist())\n",
    "\n",
    "features = np.asarray(features)\n",
    "human_placements = beats_df['placement'].to_numpy()\n",
    "\n",
    "# output = np.concatenate((beat_frames_with_mel, beat_frames_with_chroma), axis=0)\n",
    "\n",
    "# print(len(features))\n",
    "# print(len(beats_df))\n",
    "# print(len(human_placements))\n",
    "# print(human_placements[:50])\n",
    "# print(features[0:2])\n",
    "# print(features)\n",
    "# print(beats_df.iloc[0:40, 0:3])\n",
    "# print(beats_df.dtypes)\n",
    "# print(librosa.time_to_frames(length))\n",
    "# print(len(beat_frames_with_quarter))    # Do match\n",
    "# print(len(output[9]))                   # Do match\n",
    "# print(len(beat_frames))      \n",
    "# print(len(beat_frames_with_chroma))\n",
    "# print(beat_frames_with_quarter[:18])\n",
    "# print(beat_frames[:10])\n",
    "# print(output[10][:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_points = librosa.core.time_to_frames(time_points, sr=sr)\n",
    "beat_frames_with_mel = librosa.util.sync(melspectrogram, frame_points, aggregate=np.median)\n",
    "beat_frames_with_chroma = librosa.util.sync(chromagram, frame_points, aggregate=np.median)\n",
    "output = np.concatenate((beat_frames_with_mel, beat_frames_with_chroma), axis=0)\n",
    "# print(len(beat_frames_with_mel))\n",
    "# print(len(beat_frames_with_chroma))\n",
    "print(len(frame_points))    # DONT MATCH????\n",
    "print(len(output[9]))       # DONT MATCH????\n",
    "print(output[10][:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sequences, placements_for_seq = split_into_sequences(features, human_placements, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4530\n25\n4019\n512\n25\n4019\n[[0.00000000e+00 0.00000000e+00 2.30849841e-23 ... 8.53028059e-01\n  8.19255352e-01 8.36956620e-01]\n [0.00000000e+00 0.00000000e+00 2.30849841e-23 ... 8.53028059e-01\n  8.19255352e-01 8.36956620e-01]\n [0.00000000e+00 0.00000000e+00 2.30849841e-23 ... 8.53028059e-01\n  8.19255352e-01 8.36956620e-01]\n ...\n [4.64399093e-02 0.00000000e+00 2.60810107e-01 ... 9.08461094e-01\n  8.72298896e-01 8.52924228e-01]\n [0.00000000e+00 0.00000000e+00 2.60810107e-01 ... 9.08461094e-01\n  8.72298896e-01 8.52924228e-01]\n [2.32199546e-02 0.00000000e+00 2.60810107e-01 ... 9.08461094e-01\n  8.72298896e-01 8.52924228e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(len(features))\n",
    "print(len(features[0]))\n",
    "print(len(feature_sequences))\n",
    "print(len(feature_sequences[0]))\n",
    "print(len(feature_sequences[0][0]))\n",
    "print(len(placements_for_seq))\n",
    "print(feature_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(4019, 512, 25)\n"
     ]
    }
   ],
   "source": [
    "print(feature_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Saved_Features/test/features.txt'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-454-c1b2e6d016a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Saved_Features/test/features.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Test\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Saved_Features/test/features.txt'"
     ]
    }
   ],
   "source": [
    "with open('Saved_Features/test/features.txt', 'w') as f:\n",
    "    f.write(\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test.npy', feature_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "OrderedDict([('features', array([[[ 0.00000000e+00,  0.00000000e+00,  2.30849841e-23, ...,\n          8.53028059e-01,  8.19255352e-01,  8.36956620e-01],\n        [ 0.00000000e+00,  0.00000000e+00,  2.30849841e-23, ...,\n          8.53028059e-01,  8.19255352e-01,  8.36956620e-01],\n        [ 0.00000000e+00,  0.00000000e+00,  2.30849841e-23, ...,\n          8.53028059e-01,  8.19255352e-01,  8.36956620e-01],\n        ...,\n        [ 4.64399093e-02,  0.00000000e+00,  2.60810107e-01, ...,\n          9.08461094e-01,  8.72298896e-01,  8.52924228e-01],\n        [ 0.00000000e+00,  0.00000000e+00,  2.60810107e-01, ...,\n          9.08461094e-01,  8.72298896e-01,  8.52924228e-01],\n        [ 2.32199546e-02,  0.00000000e+00,  2.60810107e-01, ...,\n          9.08461094e-01,  8.72298896e-01,  8.52924228e-01]],\n\n       [[ 0.00000000e+00,  0.00000000e+00,  2.30849841e-23, ...,\n          8.53028059e-01,  8.19255352e-01,  8.36956620e-01],\n        [ 0.00000000e+00,  0.00000000e+00,  2.30849841e-23, ...,\n          8.53028059e-01,  8.19255352e-01,  8.36956620e-01],\n        [ 0.00000000e+00,  0.00000000e+00,  2.30849841e-23, ...,\n          8.53028059e-01,  8.19255352e-01,  8.36956620e-01],\n        ...,\n        [ 0.00000000e+00,  0.00000000e+00,  2.60810107e-01, ...,\n          9.08461094e-01,  8.72298896e-01,  8.52924228e-01],\n        [ 2.32199546e-02,  0.00000000e+00,  2.60810107e-01, ...,\n          9.08461094e-01,  8.72298896e-01,  8.52924228e-01],\n        [ 4.64399093e-02,  0.00000000e+00,  2.45310378e+00, ...,\n          9.50684845e-01,  9.34511185e-01,  9.17855620e-01]],\n\n       [[ 0.00000000e+00,  0.00000000e+00,  2.30849841e-23, ...,\n          8.53028059e-01,  8.19255352e-01,  8.36956620e-01],\n        [ 0.00000000e+00,  0.00000000e+00,  2.30849841e-23, ...,\n          8.53028059e-01,  8.19255352e-01,  8.36956620e-01],\n        [ 0.00000000e+00,  0.00000000e+00,  3.87054314e-14, ...,\n          7.01185167e-01,  6.80216849e-01,  7.84609914e-01],\n        ...,\n        [ 2.32199546e-02,  0.00000000e+00,  2.60810107e-01, ...,\n          9.08461094e-01,  8.72298896e-01,  8.52924228e-01],\n        [ 4.64399093e-02,  0.00000000e+00,  2.45310378e+00, ...,\n          9.50684845e-01,  9.34511185e-01,  9.17855620e-01],\n        [ 6.96598639e-02,  0.00000000e+00,  2.45310378e+00, ...,\n          9.50684845e-01,  9.34511185e-01,  9.17855620e-01]],\n\n       ...,\n\n       [[ 4.64399093e-02,  0.00000000e+00,  5.18960333e+00, ...,\n          7.74391651e-01,  6.63668513e-01,  2.71326661e-01],\n        [ 0.00000000e+00,  0.00000000e+00,  5.18960333e+00, ...,\n          7.74391651e-01,  6.63668513e-01,  2.71326661e-01],\n        [ 2.32199546e-02,  0.00000000e+00,  5.18960333e+00, ...,\n          7.74391651e-01,  6.63668513e-01,  2.71326661e-01],\n        ...,\n        [ 8.35918367e-01,  0.00000000e+00,  6.82085602e-06, ...,\n          2.05423087e-01,  5.29983282e-01,  1.00000000e+00],\n        [ 8.35918367e-01,  0.00000000e+00,  6.82085602e-06, ...,\n          2.05423087e-01,  5.29983282e-01,  1.00000000e+00],\n        [ 8.35918367e-01,  0.00000000e+00,  6.82085602e-06, ...,\n          2.05423087e-01,  5.29983282e-01,  1.00000000e+00]],\n\n       [[ 0.00000000e+00,  0.00000000e+00,  5.18960333e+00, ...,\n          7.74391651e-01,  6.63668513e-01,  2.71326661e-01],\n        [ 2.32199546e-02,  0.00000000e+00,  5.18960333e+00, ...,\n          7.74391651e-01,  6.63668513e-01,  2.71326661e-01],\n        [ 4.64399093e-02,  0.00000000e+00,  5.18960333e+00, ...,\n          7.74391651e-01,  6.63668513e-01,  2.71326661e-01],\n        ...,\n        [ 8.35918367e-01,  0.00000000e+00,  6.82085602e-06, ...,\n          2.05423087e-01,  5.29983282e-01,  1.00000000e+00],\n        [ 8.35918367e-01,  0.00000000e+00,  6.82085602e-06, ...,\n          2.05423087e-01,  5.29983282e-01,  1.00000000e+00],\n        [ 8.59138322e-01,  0.00000000e+00,  2.37200675e-06, ...,\n          4.77202713e-01,  6.48056209e-01,  8.27999830e-01]],\n\n       [[ 2.32199546e-02,  0.00000000e+00,  5.18960333e+00, ...,\n          7.74391651e-01,  6.63668513e-01,  2.71326661e-01],\n        [ 4.64399093e-02,  0.00000000e+00,  5.18960333e+00, ...,\n          7.74391651e-01,  6.63668513e-01,  2.71326661e-01],\n        [ 0.00000000e+00,  0.00000000e+00,  5.88221431e-01, ...,\n          3.61085236e-01,  2.56216586e-01,  1.83728784e-01],\n        ...,\n        [ 8.35918367e-01,  0.00000000e+00,  6.82085602e-06, ...,\n          2.05423087e-01,  5.29983282e-01,  1.00000000e+00],\n        [ 8.59138322e-01,  0.00000000e+00,  2.37200675e-06, ...,\n          4.77202713e-01,  6.48056209e-01,  8.27999830e-01],\n        [-9.01863039e+01,  0.00000000e+00,  4.28342131e-22, ...,\n          5.79528451e-01,  6.74606383e-01,  9.40546870e-01]]])), ('target', array([0, 0, 0, ..., 0, 7, 0], dtype=int64))])\n<class 'builtin_function_or_method'>\n<built-in method keys of collections.OrderedDict object at 0x0000022FDEE17BC0>\n"
     ]
    }
   ],
   "source": [
    "test_dict = OrderedDict({'features' : feature_sequences, 'target' : placements_for_seq})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}