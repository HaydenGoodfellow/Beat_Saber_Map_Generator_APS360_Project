{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('Hayde': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "58c7c12f8aebb3b799780e2a952497b720e238e6f3ceeaa6cb2f46c86be22697"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting markovify\n  Downloading markovify-0.9.0.tar.gz (27 kB)\nCollecting unidecode\n  Downloading Unidecode-1.2.0-py2.py3-none-any.whl (241 kB)\nBuilding wheels for collected packages: markovify\n  Building wheel for markovify (setup.py): started\n  Building wheel for markovify (setup.py): finished with status 'done'\n  Created wheel for markovify: filename=markovify-0.9.0-py3-none-any.whl size=18480 sha256=49d25cf7bda77d9f06053bb47d5dac1545f31b294f73f461dd7de82e636a1919\n  Stored in directory: c:\\users\\hayde\\appdata\\local\\pip\\cache\\wheels\\cf\\19\\41\\0f8707b2305726fadbd92649dcdb28d98a04e159eb24dd72f0\nSuccessfully built markovify\nInstalling collected packages: unidecode, markovify\nSuccessfully installed markovify-0.9.0 unidecode-1.2.0\nNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from zipfile import ZipFile\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import markovify as mk\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"\\\\Map_Processing\")\n",
    "\n",
    "from analyze_notes import get_notes_as_strings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get maps dataframe from the pickle file\n",
    "maps_df = pd.read_pickle(\"../Data_Gather_Filter_Download/downloaded_maps_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of training maps: 700. Numer of validation maps: 150. Number of testing maps: 150\n"
     ]
    }
   ],
   "source": [
    "#==================================== Dataset Settings ====================================#\n",
    "# Note: We have to use a subset of all our songs as 15k maps would take days to train\n",
    "total_data_size = 1000  # Number of maps to use in across all datasets\n",
    "val_split = 0.15        # Percentage of data put into validation set\n",
    "test_split = 0.15       # Percentage of data put into testing set\n",
    "#==========================================================================================#\n",
    "\n",
    "# Split our data into training and test/val which we will split again\n",
    "train_data, val_test_data = train_test_split(maps_df[:total_data_size], test_size=val_split + test_split)\n",
    "\n",
    "# Split the validation and testing data apart into their own respective sets\n",
    "val_data, test_data = train_test_split(val_test_data, test_size=(test_split/(val_split + test_split)))\n",
    "\n",
    "print(\"Number of training maps: {}. Numer of validation maps: {}. Number of testing maps: {}\".format(len(train_data), len(val_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data into format that can be read by model\n",
    "all_train_songs_as_strings = [] # List of list of strings which represent notes\n",
    "for map_data in train_data.itertuples():\n",
    "    file_path = map_data.file_path\n",
    "    if file_path != \"NOT_FOUND\":\n",
    "        with ZipFile(\"../Data_Gather_Filter_Download/{}\".format(file_path)) as folder:\n",
    "            try:\n",
    "                # Open the dat file for the difficulty\n",
    "                if map_data.difficulty == 'expert':\n",
    "                    difficulty_dat = \"Expert.dat\"\n",
    "                else:\n",
    "                    difficulty_dat = \"ExpertPlus.dat\"\n",
    "                with folder.open(difficulty_dat) as diff_dat:\n",
    "                    dat_json = json.load(diff_dat)\n",
    "                    # Get the notes at time points in a song as a string \n",
    "                    notes_as_strings = get_notes_as_strings(dat_json)\n",
    "                    all_train_songs_as_strings.append(notes_as_strings)\n",
    "            except Exception as e:\n",
    "                print(e, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 502 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#==================================== Training Settings ===================================#\n",
    "state_size = 4      # The number of prior notes it will consider with every note\n",
    "#==========================================================================================#\n",
    "# Train the Markov Chain\n",
    "markov_chain = mk.Chain(all_train_songs_as_strings, state_size=state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sequence of notes from the HMM and see how they are\n",
    "num_notes = 100\n",
    "notes_list = markov_chain.walk()\n",
    "while len(notes_list) < num_notes:\n",
    "    notes_list = markov_chain.walk() # Adds a new note every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0,1,0,0,9,9,9,9,1,1,0,3,9,9,9,9\n0,0,1,0,9,9,9,9,1,0,1,3,9,9,9,9\n0,7,0,0,9,9,9,9,1,7,0,3,9,9,9,9\n0,2,0,0,9,9,9,9,9,9,9,9,9,9,9,9\n0,3,1,0,9,9,9,9,9,9,9,9,9,9,9,9\n"
     ]
    }
   ],
   "source": [
    "for notes in notes_list[:5]:\n",
    "    print(notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}