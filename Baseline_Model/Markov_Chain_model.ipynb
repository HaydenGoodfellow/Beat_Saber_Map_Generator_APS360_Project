{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "58c7c12f8aebb3b799780e2a952497b720e238e6f3ceeaa6cb2f46c86be22697"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from zipfile import ZipFile\n",
    "from collections import OrderedDict\n",
    "import librosa\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import markovify as mk\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"\\\\Map_Processing\")\n",
    "    sys.path.append(module_path+\"\\\\Note_Timing\")\n",
    "\n",
    "# import importlib\n",
    "# importlib.reload(sys.modules['analyze_notes'])\n",
    "# importlib.reload(sys.modules['onset_detection'])\n",
    "\n",
    "from analyze_notes import get_notes_as_strings \n",
    "from onset_detection import get_onset_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get maps dataframe from the pickle file\n",
    "maps_df = pd.read_pickle(\"../Data_Gather_Filter_Download/downloaded_maps_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load markov moodel from file if already trained\n",
    "try:\n",
    "    with open(\"markov_model_64_state.json\", 'r') as markov_file:\n",
    "        markov_chain_str = json.load(markov_file)\n",
    "        markov_chain = mk.Chain.from_json(markov_chain_str)\n",
    "except Exception as e:\n",
    "    print(\"Couldn't open markov model from file. Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================== Dataset Settings ====================================#\n",
    "# Note: We have to use a subset of all our songs as 15k maps would take days to train\n",
    "total_data_size = 15000 # Number of maps to use in across all datasets\n",
    "val_split = 0.01        # Percentage of data put into validation set\n",
    "test_split = 0.01       # Percentage of data put into testing set\n",
    "#==========================================================================================#\n",
    "\n",
    "# Split our data into training and test/val which we will split again\n",
    "train_data, val_test_data = train_test_split(maps_df[:total_data_size], test_size=val_split + test_split)\n",
    "\n",
    "# Split the validation and testing data apart into their own respective sets\n",
    "val_data, test_data = train_test_split(val_test_data, test_size=(test_split/(val_split + test_split)))\n",
    "\n",
    "print(\"Number of training maps: {}. Numer of validation maps: {}. Number of testing maps: {}\".format(len(train_data), len(val_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Get training data into format that can be read by model\n",
    "all_train_songs_as_strings = [] # List of list of strings which represent notes\n",
    "for map_data in train_data.itertuples():\n",
    "    file_path = map_data.file_path\n",
    "    if file_path != \"NOT_FOUND\":\n",
    "        with ZipFile(\"../Data_Gather_Filter_Download/{}\".format(file_path)) as folder:\n",
    "            try:\n",
    "                # Open the dat file for the difficulty\n",
    "                if map_data.difficulty == 'expert':\n",
    "                    difficulty_dat = \"Expert.dat\"\n",
    "                else:\n",
    "                    difficulty_dat = \"ExpertPlus.dat\"\n",
    "                with folder.open(difficulty_dat) as diff_dat:\n",
    "                    dat_json = json.load(diff_dat)\n",
    "                    # Get the notes at time points in a song as a string \n",
    "                    notes_as_strings = get_notes_as_strings(dat_json)\n",
    "                    all_train_songs_as_strings.append(notes_as_strings)\n",
    "            except Exception as e:\n",
    "                print(e, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#==================================== Training Settings ===================================#\n",
    "state_size = 128         # The number of prior notes it will consider with every note\n",
    "#==========================================================================================#\n",
    "# Train the Markov Chain\n",
    "markov_chain = mk.Chain(all_train_songs_as_strings, state_size=state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"markov_model_64_state.json\", 'w') as f:\n",
    "    json.dump(markov_chain.to_json(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns file path to folder containing all files needed to play song made by model\n",
    "def get_map_from_song(song_file, markov_chain, output_file_path='Generated_Maps/Expert.dat', start_time=2, bpm=0):\n",
    "    try:\n",
    "        with open(\"markov_model.json\", 'r') as markov_file:\n",
    "            markov_chain_str = json.load(markov_file)\n",
    "            markov_chain = mk.Chain.from_json(markov_chain_str)\n",
    "    except Exception as e:\n",
    "        print(\"Couldn't open markov model from file. Error:\", e)\n",
    "        # return\n",
    "\n",
    "    # Get the onset times where we will place notes\n",
    "    onset_times = get_onset_times(song_file, min_sep=0.05)\n",
    "    num_before = len(onset_times)\n",
    "    onset_times = np.delete(onset_times, np.where(onset_times <= start_time))\n",
    "    print(\"Removed {} onset times for being before the specified start time\".format(num_before - len(onset_times)))\n",
    "    # If the bpm is not provided then we calculate it ourselves\n",
    "    if bpm == 0:\n",
    "        y, samp_rate = librosa.load(song_file)\n",
    "        bpm = librosa.beat.tempo(y=y, sr=samp_rate)\n",
    "        print(\"Got a bpm of {}\".format(bpm))\n",
    "    # Determine the notes we should place\n",
    "    notes_list = markov_chain.walk()\n",
    "    while len(notes_list) < len(onset_times):\n",
    "        notes_list = markov_chain.walk()\n",
    "    \n",
    "    # Create dictionary with time key and notes values\n",
    "    notes_at_times = OrderedDict(zip(onset_times, notes_list))\n",
    "    notes_as_json = convert_notes_string_to_valid_json(notes_at_times, bpm)\n",
    "    with open(output_file_path, 'w') as dat_file:\n",
    "        dat_data = {\"_version\": \"2.2.0\",\n",
    "                    \"_customData\": {\n",
    "                        \"_time\": '',\n",
    "                        \"_BPMChanges\": [],\n",
    "                        \"_bookmarks\": []\n",
    "                        },\n",
    "                    \"_events\": [],\n",
    "                    \"_notes\": notes_as_json,\n",
    "                    \"_obstacles\": [],\n",
    "                    \"_waypoints\": []\n",
    "                    }\n",
    "        json.dump(dat_data, dat_file)\n",
    "    \n",
    "    print(\"Number of notes placed: {}\\nNumber of unique note placements: {}\\nApprox. notes per second: {}\".format(\n",
    "            len(notes_as_json),\n",
    "            len(set(notes_list)),\n",
    "            len(notes_as_json) / np.amax(onset_times)\n",
    "            )\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in ordered dictonary mapping time to notes string and returns list of json\n",
    "def convert_notes_string_to_valid_json(notes_at_times, bpm):\n",
    "    list_of_jsons = []\n",
    "    for time_point, notes_string in notes_at_times.items():\n",
    "        notes_list = [int(x) for x in notes_string.split(',')]\n",
    "        assert len(notes_list) == 16\n",
    "        # Go over the 4 notes in the list and \n",
    "        for note_num in range(4):\n",
    "            try:\n",
    "                note_info = notes_list[4 * note_num : 4 * (note_num + 1)]\n",
    "                if note_info[0] not in [0, 1]: # No note\n",
    "                    continue\n",
    "                colour = note_info[0]\n",
    "                direction = note_info[1]\n",
    "                row = note_info[2]\n",
    "                col = note_info[3]\n",
    "                note_json = {\"_time\": (time_point / 60) * bpm, # Convert to beat timing\n",
    "                            \"_lineIndex\": col,\n",
    "                            \"_lineLayer\": row,\n",
    "                            \"_type\": colour,\n",
    "                            \"_cutDirection\": direction}\n",
    "                list_of_jsons.append(note_json)\n",
    "            except Exception as e:\n",
    "                print(e, \"note_num {}, max index {}\".format(note_num, 4 * (note_num + 1)))\n",
    "    return list_of_jsons"
   ]
  }
 ]
}