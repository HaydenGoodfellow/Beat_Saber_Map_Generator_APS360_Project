{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "58c7c12f8aebb3b799780e2a952497b720e238e6f3ceeaa6cb2f46c86be22697"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting markovify\n  Downloading markovify-0.9.0.tar.gz (27 kB)\nCollecting unidecode\n  Downloading Unidecode-1.2.0-py2.py3-none-any.whl (241 kB)\nBuilding wheels for collected packages: markovify\n  Building wheel for markovify (setup.py): started\n  Building wheel for markovify (setup.py): finished with status 'done'\n  Created wheel for markovify: filename=markovify-0.9.0-py3-none-any.whl size=18480 sha256=49d25cf7bda77d9f06053bb47d5dac1545f31b294f73f461dd7de82e636a1919\n  Stored in directory: c:\\users\\hayde\\appdata\\local\\pip\\cache\\wheels\\cf\\19\\41\\0f8707b2305726fadbd92649dcdb28d98a04e159eb24dd72f0\nSuccessfully built markovify\nInstalling collected packages: unidecode, markovify\nSuccessfully installed markovify-0.9.0 unidecode-1.2.0\nNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from zipfile import ZipFile\n",
    "from collections import OrderedDict\n",
    "import librosa\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import markovify as mk\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"\\\\Map_Processing\")\n",
    "    sys.path.append(module_path+\"\\\\Note_Timing\")\n",
    "\n",
    "# import importlib\n",
    "# importlib.reload(sys.modules['analyze_notes'])\n",
    "# importlib.reload(sys.modules['onset_detection'])\n",
    "\n",
    "from analyze_notes import get_notes_as_strings \n",
    "from onset_detection import get_onset_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get maps dataframe from the pickle file\n",
    "maps_df = pd.read_pickle(\"../Data_Gather_Filter_Download/downloaded_maps_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load markov moodel from file if already trained\n",
    "try:\n",
    "    with open(\"markov_model.json\", 'r') as markov_file:\n",
    "        markov_chain_str = json.load(markov_file)\n",
    "        markov_chain = mk.Chain.from_json(markov_chain_str)\n",
    "except Exception as e:\n",
    "    print(\"Couldn't open markov model from file. Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of training maps: 14700. Numer of validation maps: 150. Number of testing maps: 150\n"
     ]
    }
   ],
   "source": [
    "#==================================== Dataset Settings ====================================#\n",
    "# Note: We have to use a subset of all our songs as 15k maps would take days to train\n",
    "total_data_size = 15000 # Number of maps to use in across all datasets\n",
    "val_split = 0.01        # Percentage of data put into validation set\n",
    "test_split = 0.01       # Percentage of data put into testing set\n",
    "#==========================================================================================#\n",
    "\n",
    "# Split our data into training and test/val which we will split again\n",
    "train_data, val_test_data = train_test_split(maps_df[:total_data_size], test_size=val_split + test_split)\n",
    "\n",
    "# Split the validation and testing data apart into their own respective sets\n",
    "val_data, test_data = train_test_split(val_test_data, test_size=(test_split/(val_split + test_split)))\n",
    "\n",
    "print(\"Number of training maps: {}. Numer of validation maps: {}. Number of testing maps: {}\".format(len(train_data), len(val_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 8min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Get training data into format that can be read by model\n",
    "all_train_songs_as_strings = [] # List of list of strings which represent notes\n",
    "for map_data in train_data.itertuples():\n",
    "    file_path = map_data.file_path\n",
    "    if file_path != \"NOT_FOUND\":\n",
    "        with ZipFile(\"../Data_Gather_Filter_Download/{}\".format(file_path)) as folder:\n",
    "            try:\n",
    "                # Open the dat file for the difficulty\n",
    "                if map_data.difficulty == 'expert':\n",
    "                    difficulty_dat = \"Expert.dat\"\n",
    "                else:\n",
    "                    difficulty_dat = \"ExpertPlus.dat\"\n",
    "                with folder.open(difficulty_dat) as diff_dat:\n",
    "                    dat_json = json.load(diff_dat)\n",
    "                    # Get the notes at time points in a song as a string \n",
    "                    notes_as_strings = get_notes_as_strings(dat_json)\n",
    "                    all_train_songs_as_strings.append(notes_as_strings)\n",
    "            except Exception as e:\n",
    "                print(e, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#==================================== Training Settings ===================================#\n",
    "state_size = 64         # The number of prior notes it will consider with every note\n",
    "#==========================================================================================#\n",
    "# Train the Markov Chain\n",
    "markov_chain = mk.Chain(all_train_songs_as_strings, state_size=state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "MemoryError",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-8417ff1c388d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"markov_model_64_state.json\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkov_chain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\markovify\\chain.py\u001b[0m in \u001b[0;36mto_json\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[0mDump\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mJSON\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mloading\u001b[0m \u001b[0mlater\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \"\"\"\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mindent\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mseparators\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m         default is None and not sort_keys and not kw):\n\u001b[1;32m--> 231\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[1;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open(\"markov_model_64_state.json\", 'w') as f:\n",
    "    json.dump(markov_chain.to_json(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns file path to folder containing all files needed to play song made by model\n",
    "def get_map_from_song(song_file, output_file_path='Generated_Maps/Expert.dat', start_time=2, bpm=0):\n",
    "    try:\n",
    "        with open(\"markov_model.json\", 'r') as markov_file:\n",
    "            markov_chain_str = json.load(markov_file)\n",
    "            markov_chain = mk.Chain.from_json(markov_chain_str)\n",
    "    except Exception as e:\n",
    "        print(\"Couldn't open markov model from file. Error:\", e)\n",
    "        return\n",
    "\n",
    "    # Get the onset times where we will place notes\n",
    "    onset_times = get_onset_times(song_file, min_sep=0.1)\n",
    "    num_before = len(onset_times)\n",
    "    onset_times = np.delete(onset_times, np.where(onset_times <= start_time))\n",
    "    print(\"Removed {} onset times for being before the specified start time\".format(num_before - len(onset_times)))\n",
    "    # If the bpm is not provided then we calculate it ourselves\n",
    "    if bpm == 0:\n",
    "        y, samp_rate = librosa.load(song_file)\n",
    "        bpm = librosa.beat.tempo(y=y, sr=samp_rate)\n",
    "        print(\"Got a bpm of {}\".format(bpm))\n",
    "    # Determine the notes we should place\n",
    "    notes_list = markov_chain.walk()\n",
    "    while len(notes_list) < len(onset_times):\n",
    "        notes_list = markov_chain.walk()\n",
    "    \n",
    "    # Create dictionary with time key and notes values\n",
    "    notes_at_times = OrderedDict(zip(onset_times, notes_list))\n",
    "    notes_as_json = convert_notes_string_to_valid_json(notes_at_times, bpm)\n",
    "    with open(output_file_path, 'w') as dat_file:\n",
    "        dat_data = {\"_version\": \"2.2.0\",\n",
    "                    \"_customData\": {\n",
    "                        \"_time\": '',\n",
    "                        \"_BPMChanges\": [],\n",
    "                        \"_bookmarks\": []\n",
    "                        },\n",
    "                    \"_events\": [],\n",
    "                    \"_notes\": notes_as_json,\n",
    "                    \"_obstacles\": [],\n",
    "                    \"_waypoints\": []\n",
    "                    }\n",
    "        json.dump(dat_data, dat_file)\n",
    "    \n",
    "    print(\"Number of notes placed: {}\\nNumber of unique note placements: {}\\nApprox. notes per second: {}\".format(\n",
    "            len(notes_as_json),\n",
    "            len(set(notes_list)),\n",
    "            len(notes_as_json) / np.amax(onset_times)\n",
    "            )\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in ordered dictonary mapping time to notes string and returns list of json\n",
    "def convert_notes_string_to_valid_json(notes_at_times, bpm):\n",
    "    list_of_jsons = []\n",
    "    for time_point, notes_string in notes_at_times.items():\n",
    "        notes_list = [int(x) for x in notes_string.split(',')]\n",
    "        assert len(notes_list) == 16\n",
    "        # Go over the 4 notes in the list and \n",
    "        for note_num in range(4):\n",
    "            try:\n",
    "                note_info = notes_list[4 * note_num : 4 * (note_num + 1)]\n",
    "                if note_info[0] not in [0, 1]: # No note\n",
    "                    continue\n",
    "                colour = note_info[0]\n",
    "                direction = note_info[1]\n",
    "                row = note_info[2]\n",
    "                col = note_info[3]\n",
    "                note_json = {\"_time\": (time_point / 60) * bpm, # Convert to beat timing\n",
    "                            \"_lineIndex\": col,\n",
    "                            \"_lineLayer\": row,\n",
    "                            \"_type\": colour,\n",
    "                            \"_cutDirection\": direction}\n",
    "                list_of_jsons.append(note_json)\n",
    "            except Exception as e:\n",
    "                print(e, \"note_num {}, max index {}\".format(note_num, 4 * (note_num + 1)))\n",
    "    return list_of_jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Removed 12 onset times for being within 0.1s of the next note\nRemoved 0 onset times for being before the specified start time\nNumber of notes placed: 405\nNumber of unique note placements: 104\nApprox. notes per second: 4.475723513279445\n"
     ]
    }
   ],
   "source": [
    "# Test our model\n",
    "song_file_name = \"(706a)_Redo_(TV_Size)_ReZero_Opening_-_Konomi_Suzuki\"\n",
    "# (7067)_Sorairo_Days_(TV_Size)_Gurren_Lagann_Opening_-_Shoko_Nakagawa\n",
    "\n",
    "with ZipFile('../Data_Gather_Filter_Download/Zip_Songs_Data/{}.zip'.format(song_file_name)) as folder:\n",
    "    folder.extract('song.egg')\n",
    "    get_map_from_song('song.egg', start_time=0, bpm=190)\n",
    "    os.remove('song.egg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sequence of notes from the HMM and see how they are\n",
    "num_notes_test = 100\n",
    "notes_list_test = markov_chain.walk()\n",
    "while len(notes_list_test) < num_notes_test:\n",
    "    notes_list_test = markov_chain.walk() # Adds a new note every time"
   ]
  }
 ]
}